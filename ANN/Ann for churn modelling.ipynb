{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7SKXfTJjFlO",
        "outputId": "45691b8b-22b6-4c65-8706-5cdd81891c8b"
      },
      "source": [
        "!pip install colab_ssh --upgrade\n",
        "\n",
        "# Asking the Token\n",
        "token = input('Pls enter your Auth-Token here: ' )\n",
        "password = input('Pls enter your password here: ' )\n",
        "\n",
        "from colab_ssh import launch_ssh\n",
        "launch_ssh(token, password)\n",
        "\n",
        "import time\n",
        "while True:\n",
        "time.sleep(300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: colab_ssh in /usr/local/lib/python3.7/dist-packages (0.3.15)\n",
            "Pls enter your Auth-Token here: 1q6kmwgdRpM5Rx1TY00M2E5nzFX_XUMEME7wxn6o2fdceu8E\n",
            "Pls enter your password here: root\n",
            "Warning: Due to some issues with ngrok on Google Colab, reported in the issue https://github.com/WassimBenzarti/colab-ssh/issues/45, \n",
            "we highly recommend that update your code by following this documentation https://github.com/WassimBenzarti/colab-ssh#getting-started\n",
            "Successfully running 4.tcp.ngrok.io:11352\n",
            "[Optional] You can also connect with VSCode SSH Remote extension using this configuration:\n",
            "\n",
            "\tHost google_colab_ssh\n",
            "\t\tHostName 4.tcp.ngrok.io\n",
            "\t\tUser root\n",
            "\t\tPort 11352\n",
            "\t  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qmc2HAUWNs_"
      },
      "source": [
        "# Importing the libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "D07FTenKXV1r",
        "outputId": "6aec6f85-6263-4c6a-ce66-5ed8895f3906"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c0598e1a-fe8c-4fa3-8b29-05459e3e7180\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c0598e1a-fe8c-4fa3-8b29-05459e3e7180\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving file_churn_modelling.csv to file_churn_modelling.csv\n",
            "/content\n",
            "file_churn_modelling.csv  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "CYMf6szVZaz_",
        "outputId": "299cf9df-2c27-444e-b381-984276cdf23f"
      },
      "source": [
        "# Importing the dataset\n",
        "dataset = pd.read_csv('/content/file_churn_modelling.csv')\n",
        "dataset\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowNumber</th>\n",
              "      <th>CustomerId</th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>15634602</td>\n",
              "      <td>Hargrave</td>\n",
              "      <td>619</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>15647311</td>\n",
              "      <td>Hill</td>\n",
              "      <td>608</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>15619304</td>\n",
              "      <td>Onio</td>\n",
              "      <td>502</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>15701354</td>\n",
              "      <td>Boni</td>\n",
              "      <td>699</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>15737888</td>\n",
              "      <td>Mitchell</td>\n",
              "      <td>850</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9996</td>\n",
              "      <td>15606229</td>\n",
              "      <td>Obijiaku</td>\n",
              "      <td>771</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>39</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>96270.64</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9997</td>\n",
              "      <td>15569892</td>\n",
              "      <td>Johnstone</td>\n",
              "      <td>516</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>10</td>\n",
              "      <td>57369.61</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101699.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9998</td>\n",
              "      <td>15584532</td>\n",
              "      <td>Liu</td>\n",
              "      <td>709</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>36</td>\n",
              "      <td>7</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>42085.58</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9999</td>\n",
              "      <td>15682355</td>\n",
              "      <td>Sabbatini</td>\n",
              "      <td>772</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Male</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>75075.31</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>92888.52</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>10000</td>\n",
              "      <td>15628319</td>\n",
              "      <td>Walker</td>\n",
              "      <td>792</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>130142.79</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38190.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows Ã— 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      RowNumber  CustomerId    Surname  ...  IsActiveMember EstimatedSalary Exited\n",
              "0             1    15634602   Hargrave  ...               1       101348.88      1\n",
              "1             2    15647311       Hill  ...               1       112542.58      0\n",
              "2             3    15619304       Onio  ...               0       113931.57      1\n",
              "3             4    15701354       Boni  ...               0        93826.63      0\n",
              "4             5    15737888   Mitchell  ...               1        79084.10      0\n",
              "...         ...         ...        ...  ...             ...             ...    ...\n",
              "9995       9996    15606229   Obijiaku  ...               0        96270.64      0\n",
              "9996       9997    15569892  Johnstone  ...               1       101699.77      0\n",
              "9997       9998    15584532        Liu  ...               1        42085.58      1\n",
              "9998       9999    15682355  Sabbatini  ...               0        92888.52      1\n",
              "9999      10000    15628319     Walker  ...               0        38190.78      0\n",
              "\n",
              "[10000 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GpVUtsCaW4D"
      },
      "source": [
        "# Creating Features and Label column\n",
        "\n",
        "X = dataset.iloc[:, 3:13]\n",
        "y = dataset.iloc[:, 13]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QLZ9zWaWN1H"
      },
      "source": [
        "#Create dummy variables\n",
        "geography=pd.get_dummies(X[\"Geography\"],drop_first=True)\n",
        "gender=pd.get_dummies(X['Gender'],drop_first=True)\n",
        "\n",
        "## Concatenate the Data Frames\n",
        "\n",
        "X=pd.concat([X,geography,gender],axis=1)\n",
        "\n",
        "## Drop Unnecessary columns\n",
        "X=X.drop(['Geography','Gender'],axis=1)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIf9qtQYWXSY"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIgJz4OddrtY",
        "outputId": "dd9d3930-99d4-4abb-a504-07cdb1a2ed8b"
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RkYvv8tWXbB"
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG8WlpWbWihF"
      },
      "source": [
        "# Part 2 - Now let's make the ANN!\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LeakyReLU,PReLU,ELU\n",
        "from keras.layers import Dropout\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3kENqK0WjQL",
        "outputId": "cd0cab90-5fbc-408b-b43c-a1bb421bcef0"
      },
      "source": [
        "# Initialising the ANN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'he_uniform', activation = 'relu', input_dim = 11))\n",
        "classifier.add(Dropout(0.4))\n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units= 6, kernel_initializer = 'he_uniform',activation = 'relu'))\n",
        "classifier.add(Dropout(0.4))\n",
        "\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n",
        "classifier.add(Dropout(0.4))\n",
        "\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Getting the model summary\n",
        "classifier.summary()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_55 (Dense)             (None, 6)                 72        \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 6)                 0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 6)                 42        \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 6)                 0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 1)                 7         \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 121\n",
            "Trainable params: 121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7wJCJ8WWjUY",
        "outputId": "f3774c40-7b09-4b4d-a77f-b517175cd876"
      },
      "source": [
        "# Fitting the ANN to the Training set and Start Model Training \n",
        "model_history=classifier.fit(X_train, y_train, validation_split=0.28, batch_size = 8000, epochs = 500)\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 1.8144 - accuracy: 0.6825 - val_loss: 0.4859 - val_accuracy: 0.7982\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.8626 - accuracy: 0.6701 - val_loss: 0.4859 - val_accuracy: 0.7982\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.7589 - accuracy: 0.6865 - val_loss: 0.4858 - val_accuracy: 0.7982\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.6840 - accuracy: 0.6905 - val_loss: 0.4858 - val_accuracy: 0.7982\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 1.9027 - accuracy: 0.6830 - val_loss: 0.4858 - val_accuracy: 0.7982\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7526 - accuracy: 0.6965 - val_loss: 0.4857 - val_accuracy: 0.7982\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 1.7816 - accuracy: 0.6802 - val_loss: 0.4857 - val_accuracy: 0.7982\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 1.8129 - accuracy: 0.6832 - val_loss: 0.4857 - val_accuracy: 0.7982\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 1.7700 - accuracy: 0.6910 - val_loss: 0.4857 - val_accuracy: 0.7982\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.7299 - accuracy: 0.6773 - val_loss: 0.4856 - val_accuracy: 0.7982\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7243 - accuracy: 0.6792 - val_loss: 0.4856 - val_accuracy: 0.7982\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7684 - accuracy: 0.6894 - val_loss: 0.4856 - val_accuracy: 0.7982\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7615 - accuracy: 0.6941 - val_loss: 0.4855 - val_accuracy: 0.7982\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1.8245 - accuracy: 0.6851 - val_loss: 0.4855 - val_accuracy: 0.7982\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6877 - accuracy: 0.6924 - val_loss: 0.4854 - val_accuracy: 0.7982\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 1.7484 - accuracy: 0.6877 - val_loss: 0.4854 - val_accuracy: 0.7982\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7422 - accuracy: 0.6899 - val_loss: 0.4853 - val_accuracy: 0.7982\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.7886 - accuracy: 0.6828 - val_loss: 0.4853 - val_accuracy: 0.7982\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.7933 - accuracy: 0.6894 - val_loss: 0.4853 - val_accuracy: 0.7982\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7755 - accuracy: 0.6859 - val_loss: 0.4852 - val_accuracy: 0.7982\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.7585 - accuracy: 0.6951 - val_loss: 0.4852 - val_accuracy: 0.7982\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.7399 - accuracy: 0.6851 - val_loss: 0.4851 - val_accuracy: 0.7982\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7615 - accuracy: 0.6832 - val_loss: 0.4851 - val_accuracy: 0.7982\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7959 - accuracy: 0.6797 - val_loss: 0.4850 - val_accuracy: 0.7982\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7750 - accuracy: 0.6863 - val_loss: 0.4850 - val_accuracy: 0.7982\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7341 - accuracy: 0.6925 - val_loss: 0.4849 - val_accuracy: 0.7982\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.7392 - accuracy: 0.6913 - val_loss: 0.4849 - val_accuracy: 0.7982\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7597 - accuracy: 0.6748 - val_loss: 0.4849 - val_accuracy: 0.7982\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7863 - accuracy: 0.6884 - val_loss: 0.4848 - val_accuracy: 0.7982\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7893 - accuracy: 0.6823 - val_loss: 0.4848 - val_accuracy: 0.7982\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7921 - accuracy: 0.6903 - val_loss: 0.4847 - val_accuracy: 0.7982\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7874 - accuracy: 0.6889 - val_loss: 0.4847 - val_accuracy: 0.7982\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.7600 - accuracy: 0.6899 - val_loss: 0.4847 - val_accuracy: 0.7982\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7847 - accuracy: 0.6774 - val_loss: 0.4846 - val_accuracy: 0.7982\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.8206 - accuracy: 0.6691 - val_loss: 0.4846 - val_accuracy: 0.7982\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.9267 - accuracy: 0.6854 - val_loss: 0.4845 - val_accuracy: 0.7982\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 1.6873 - accuracy: 0.6965 - val_loss: 0.4845 - val_accuracy: 0.7982\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 1.7800 - accuracy: 0.6880 - val_loss: 0.4845 - val_accuracy: 0.7982\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8345 - accuracy: 0.6767 - val_loss: 0.4844 - val_accuracy: 0.7982\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7355 - accuracy: 0.6889 - val_loss: 0.4844 - val_accuracy: 0.7982\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.8174 - accuracy: 0.6866 - val_loss: 0.4843 - val_accuracy: 0.7982\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.7004 - accuracy: 0.6932 - val_loss: 0.4843 - val_accuracy: 0.7982\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7871 - accuracy: 0.6922 - val_loss: 0.4842 - val_accuracy: 0.7982\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.6981 - accuracy: 0.6918 - val_loss: 0.4842 - val_accuracy: 0.7982\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7457 - accuracy: 0.6901 - val_loss: 0.4842 - val_accuracy: 0.7982\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.7501 - accuracy: 0.6970 - val_loss: 0.4841 - val_accuracy: 0.7982\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.8556 - accuracy: 0.6877 - val_loss: 0.4841 - val_accuracy: 0.7982\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 1.7383 - accuracy: 0.6920 - val_loss: 0.4840 - val_accuracy: 0.7982\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.8333 - accuracy: 0.6972 - val_loss: 0.4840 - val_accuracy: 0.7982\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.7083 - accuracy: 0.6849 - val_loss: 0.4839 - val_accuracy: 0.7982\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7545 - accuracy: 0.6851 - val_loss: 0.4839 - val_accuracy: 0.7982\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7818 - accuracy: 0.6979 - val_loss: 0.4839 - val_accuracy: 0.7982\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.8431 - accuracy: 0.6922 - val_loss: 0.4838 - val_accuracy: 0.7982\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7989 - accuracy: 0.6856 - val_loss: 0.4838 - val_accuracy: 0.7982\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7438 - accuracy: 0.6896 - val_loss: 0.4837 - val_accuracy: 0.7982\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7005 - accuracy: 0.6953 - val_loss: 0.4837 - val_accuracy: 0.7982\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.8161 - accuracy: 0.6863 - val_loss: 0.4837 - val_accuracy: 0.7982\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.7983 - accuracy: 0.6967 - val_loss: 0.4836 - val_accuracy: 0.7982\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 1.8212 - accuracy: 0.6950 - val_loss: 0.4836 - val_accuracy: 0.7982\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.8048 - accuracy: 0.6946 - val_loss: 0.4835 - val_accuracy: 0.7982\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6542 - accuracy: 0.6929 - val_loss: 0.4835 - val_accuracy: 0.7982\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.8126 - accuracy: 0.6878 - val_loss: 0.4835 - val_accuracy: 0.7982\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7297 - accuracy: 0.6939 - val_loss: 0.4835 - val_accuracy: 0.7982\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7137 - accuracy: 0.6962 - val_loss: 0.4835 - val_accuracy: 0.7982\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6918 - accuracy: 0.6880 - val_loss: 0.4835 - val_accuracy: 0.7982\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.7432 - accuracy: 0.6859 - val_loss: 0.4835 - val_accuracy: 0.7982\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.7672 - accuracy: 0.6948 - val_loss: 0.4835 - val_accuracy: 0.7982\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.8315 - accuracy: 0.6828 - val_loss: 0.4835 - val_accuracy: 0.7982\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.7427 - accuracy: 0.6910 - val_loss: 0.4835 - val_accuracy: 0.7982\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.8401 - accuracy: 0.6819 - val_loss: 0.4835 - val_accuracy: 0.7982\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7455 - accuracy: 0.6964 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.6906 - accuracy: 0.6911 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7764 - accuracy: 0.6903 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.8162 - accuracy: 0.6882 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7991 - accuracy: 0.6880 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8535 - accuracy: 0.6844 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7565 - accuracy: 0.6870 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7389 - accuracy: 0.6934 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7343 - accuracy: 0.6964 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.7299 - accuracy: 0.6951 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.7139 - accuracy: 0.6955 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7716 - accuracy: 0.6943 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7634 - accuracy: 0.6929 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7032 - accuracy: 0.6965 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.7627 - accuracy: 0.6957 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6882 - accuracy: 0.7019 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7622 - accuracy: 0.6938 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7691 - accuracy: 0.6969 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.7543 - accuracy: 0.6972 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7509 - accuracy: 0.6889 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7681 - accuracy: 0.6927 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7259 - accuracy: 0.6934 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7262 - accuracy: 0.6943 - val_loss: 0.4834 - val_accuracy: 0.7982\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 1.7529 - accuracy: 0.7026 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.8197 - accuracy: 0.6887 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7552 - accuracy: 0.6920 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7386 - accuracy: 0.6976 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6653 - accuracy: 0.7066 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7822 - accuracy: 0.6936 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.7629 - accuracy: 0.6944 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7200 - accuracy: 0.6896 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7313 - accuracy: 0.7012 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7655 - accuracy: 0.6950 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.7224 - accuracy: 0.6851 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.8820 - accuracy: 0.6866 - val_loss: 0.4833 - val_accuracy: 0.7982\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.7313 - accuracy: 0.6964 - val_loss: 0.4832 - val_accuracy: 0.7982\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7369 - accuracy: 0.7005 - val_loss: 0.4832 - val_accuracy: 0.7982\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7195 - accuracy: 0.6948 - val_loss: 0.4832 - val_accuracy: 0.7982\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.7942 - accuracy: 0.6972 - val_loss: 0.4832 - val_accuracy: 0.7982\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.7234 - accuracy: 0.6911 - val_loss: 0.4831 - val_accuracy: 0.7982\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.7120 - accuracy: 0.6932 - val_loss: 0.4831 - val_accuracy: 0.7982\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6843 - accuracy: 0.7024 - val_loss: 0.4831 - val_accuracy: 0.7982\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7300 - accuracy: 0.6891 - val_loss: 0.4831 - val_accuracy: 0.7982\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7204 - accuracy: 0.6908 - val_loss: 0.4830 - val_accuracy: 0.7982\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6490 - accuracy: 0.7043 - val_loss: 0.4830 - val_accuracy: 0.7982\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7257 - accuracy: 0.6984 - val_loss: 0.4830 - val_accuracy: 0.7982\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7315 - accuracy: 0.6981 - val_loss: 0.4829 - val_accuracy: 0.7982\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7290 - accuracy: 0.6870 - val_loss: 0.4829 - val_accuracy: 0.7982\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6830 - accuracy: 0.7028 - val_loss: 0.4829 - val_accuracy: 0.7982\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7893 - accuracy: 0.6918 - val_loss: 0.4828 - val_accuracy: 0.7982\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7251 - accuracy: 0.6967 - val_loss: 0.4828 - val_accuracy: 0.7982\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6905 - accuracy: 0.6986 - val_loss: 0.4828 - val_accuracy: 0.7982\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7564 - accuracy: 0.6974 - val_loss: 0.4828 - val_accuracy: 0.7982\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7428 - accuracy: 0.6984 - val_loss: 0.4827 - val_accuracy: 0.7982\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7277 - accuracy: 0.7038 - val_loss: 0.4827 - val_accuracy: 0.7982\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7496 - accuracy: 0.6927 - val_loss: 0.4827 - val_accuracy: 0.7982\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7646 - accuracy: 0.6932 - val_loss: 0.4826 - val_accuracy: 0.7982\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7143 - accuracy: 0.6908 - val_loss: 0.4826 - val_accuracy: 0.7982\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.7362 - accuracy: 0.6944 - val_loss: 0.4826 - val_accuracy: 0.7982\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7197 - accuracy: 0.7016 - val_loss: 0.4825 - val_accuracy: 0.7982\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7892 - accuracy: 0.6925 - val_loss: 0.4825 - val_accuracy: 0.7982\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7723 - accuracy: 0.6944 - val_loss: 0.4825 - val_accuracy: 0.7982\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7361 - accuracy: 0.6972 - val_loss: 0.4825 - val_accuracy: 0.7982\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6953 - accuracy: 0.7059 - val_loss: 0.4824 - val_accuracy: 0.7982\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7352 - accuracy: 0.6948 - val_loss: 0.4824 - val_accuracy: 0.7982\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.5794 - accuracy: 0.6997 - val_loss: 0.4824 - val_accuracy: 0.7982\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7532 - accuracy: 0.6960 - val_loss: 0.4823 - val_accuracy: 0.7982\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6599 - accuracy: 0.7073 - val_loss: 0.4823 - val_accuracy: 0.7982\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.7794 - accuracy: 0.6929 - val_loss: 0.4822 - val_accuracy: 0.7982\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.6720 - accuracy: 0.7024 - val_loss: 0.4822 - val_accuracy: 0.7982\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.7515 - accuracy: 0.7019 - val_loss: 0.4822 - val_accuracy: 0.7982\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7652 - accuracy: 0.7071 - val_loss: 0.4821 - val_accuracy: 0.7982\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7955 - accuracy: 0.6977 - val_loss: 0.4821 - val_accuracy: 0.7982\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7476 - accuracy: 0.6957 - val_loss: 0.4821 - val_accuracy: 0.7982\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7606 - accuracy: 0.7049 - val_loss: 0.4821 - val_accuracy: 0.7982\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7014 - accuracy: 0.7035 - val_loss: 0.4820 - val_accuracy: 0.7982\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7435 - accuracy: 0.6955 - val_loss: 0.4820 - val_accuracy: 0.7982\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.8240 - accuracy: 0.6872 - val_loss: 0.4820 - val_accuracy: 0.7982\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7408 - accuracy: 0.6988 - val_loss: 0.4819 - val_accuracy: 0.7982\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7226 - accuracy: 0.6990 - val_loss: 0.4819 - val_accuracy: 0.7982\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7312 - accuracy: 0.6953 - val_loss: 0.4819 - val_accuracy: 0.7982\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6928 - accuracy: 0.7016 - val_loss: 0.4819 - val_accuracy: 0.7982\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.7238 - accuracy: 0.6977 - val_loss: 0.4819 - val_accuracy: 0.7982\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6975 - accuracy: 0.7057 - val_loss: 0.4819 - val_accuracy: 0.7982\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6667 - accuracy: 0.7003 - val_loss: 0.4818 - val_accuracy: 0.7982\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6736 - accuracy: 0.7028 - val_loss: 0.4818 - val_accuracy: 0.7982\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7581 - accuracy: 0.7005 - val_loss: 0.4818 - val_accuracy: 0.7982\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7085 - accuracy: 0.7030 - val_loss: 0.4818 - val_accuracy: 0.7982\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6783 - accuracy: 0.7014 - val_loss: 0.4818 - val_accuracy: 0.7982\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7787 - accuracy: 0.6962 - val_loss: 0.4817 - val_accuracy: 0.7982\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7178 - accuracy: 0.6967 - val_loss: 0.4817 - val_accuracy: 0.7982\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7816 - accuracy: 0.6997 - val_loss: 0.4817 - val_accuracy: 0.7982\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.7922 - accuracy: 0.6953 - val_loss: 0.4817 - val_accuracy: 0.7982\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.7013 - accuracy: 0.7054 - val_loss: 0.4817 - val_accuracy: 0.7982\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6923 - accuracy: 0.6984 - val_loss: 0.4816 - val_accuracy: 0.7982\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6885 - accuracy: 0.7050 - val_loss: 0.4816 - val_accuracy: 0.7982\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7666 - accuracy: 0.6958 - val_loss: 0.4816 - val_accuracy: 0.7982\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7052 - accuracy: 0.7023 - val_loss: 0.4816 - val_accuracy: 0.7982\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7063 - accuracy: 0.6924 - val_loss: 0.4815 - val_accuracy: 0.7982\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7113 - accuracy: 0.7023 - val_loss: 0.4815 - val_accuracy: 0.7982\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7367 - accuracy: 0.7012 - val_loss: 0.4815 - val_accuracy: 0.7982\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6558 - accuracy: 0.7097 - val_loss: 0.4815 - val_accuracy: 0.7982\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6615 - accuracy: 0.7094 - val_loss: 0.4814 - val_accuracy: 0.7982\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.7828 - accuracy: 0.7019 - val_loss: 0.4814 - val_accuracy: 0.7982\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.7175 - accuracy: 0.7040 - val_loss: 0.4814 - val_accuracy: 0.7982\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6377 - accuracy: 0.7109 - val_loss: 0.4814 - val_accuracy: 0.7982\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7450 - accuracy: 0.7043 - val_loss: 0.4813 - val_accuracy: 0.7982\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6883 - accuracy: 0.7035 - val_loss: 0.4813 - val_accuracy: 0.7982\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6595 - accuracy: 0.7005 - val_loss: 0.4813 - val_accuracy: 0.7982\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6849 - accuracy: 0.7142 - val_loss: 0.4813 - val_accuracy: 0.7982\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6096 - accuracy: 0.7149 - val_loss: 0.4812 - val_accuracy: 0.7982\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7414 - accuracy: 0.7059 - val_loss: 0.4812 - val_accuracy: 0.7982\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.7377 - accuracy: 0.7038 - val_loss: 0.4812 - val_accuracy: 0.7982\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6825 - accuracy: 0.7069 - val_loss: 0.4811 - val_accuracy: 0.7982\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6817 - accuracy: 0.6991 - val_loss: 0.4811 - val_accuracy: 0.7982\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6874 - accuracy: 0.7014 - val_loss: 0.4811 - val_accuracy: 0.7982\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6945 - accuracy: 0.7040 - val_loss: 0.4810 - val_accuracy: 0.7982\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.7702 - accuracy: 0.7085 - val_loss: 0.4810 - val_accuracy: 0.7982\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7860 - accuracy: 0.7009 - val_loss: 0.4810 - val_accuracy: 0.7982\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7183 - accuracy: 0.7009 - val_loss: 0.4809 - val_accuracy: 0.7982\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6941 - accuracy: 0.7118 - val_loss: 0.4809 - val_accuracy: 0.7982\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7224 - accuracy: 0.7021 - val_loss: 0.4809 - val_accuracy: 0.7982\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7461 - accuracy: 0.6953 - val_loss: 0.4808 - val_accuracy: 0.7982\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7833 - accuracy: 0.6965 - val_loss: 0.4808 - val_accuracy: 0.7982\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6701 - accuracy: 0.7066 - val_loss: 0.4808 - val_accuracy: 0.7982\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6914 - accuracy: 0.7066 - val_loss: 0.4807 - val_accuracy: 0.7982\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.7459 - accuracy: 0.7045 - val_loss: 0.4807 - val_accuracy: 0.7982\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6419 - accuracy: 0.7042 - val_loss: 0.4807 - val_accuracy: 0.7982\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6769 - accuracy: 0.7040 - val_loss: 0.4806 - val_accuracy: 0.7982\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.6588 - accuracy: 0.6976 - val_loss: 0.4806 - val_accuracy: 0.7982\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6649 - accuracy: 0.7125 - val_loss: 0.4806 - val_accuracy: 0.7982\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6150 - accuracy: 0.7057 - val_loss: 0.4805 - val_accuracy: 0.7982\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.7831 - accuracy: 0.7101 - val_loss: 0.4805 - val_accuracy: 0.7982\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.7193 - accuracy: 0.7095 - val_loss: 0.4805 - val_accuracy: 0.7982\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6988 - accuracy: 0.6991 - val_loss: 0.4804 - val_accuracy: 0.7982\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.7245 - accuracy: 0.7043 - val_loss: 0.4804 - val_accuracy: 0.7982\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7068 - accuracy: 0.7174 - val_loss: 0.4804 - val_accuracy: 0.7982\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6870 - accuracy: 0.7120 - val_loss: 0.4804 - val_accuracy: 0.7982\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7108 - accuracy: 0.7073 - val_loss: 0.4803 - val_accuracy: 0.7982\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.7376 - accuracy: 0.6965 - val_loss: 0.4803 - val_accuracy: 0.7982\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7751 - accuracy: 0.6997 - val_loss: 0.4803 - val_accuracy: 0.7982\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.7289 - accuracy: 0.7068 - val_loss: 0.4803 - val_accuracy: 0.7982\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6995 - accuracy: 0.7109 - val_loss: 0.4802 - val_accuracy: 0.7982\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.6612 - accuracy: 0.7019 - val_loss: 0.4802 - val_accuracy: 0.7982\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7400 - accuracy: 0.7078 - val_loss: 0.4802 - val_accuracy: 0.7982\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7234 - accuracy: 0.7005 - val_loss: 0.4801 - val_accuracy: 0.7982\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6709 - accuracy: 0.7052 - val_loss: 0.4801 - val_accuracy: 0.7982\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7296 - accuracy: 0.7024 - val_loss: 0.4801 - val_accuracy: 0.7982\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.7053 - accuracy: 0.7052 - val_loss: 0.4800 - val_accuracy: 0.7982\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.7448 - accuracy: 0.6995 - val_loss: 0.4800 - val_accuracy: 0.7982\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.7874 - accuracy: 0.7090 - val_loss: 0.4800 - val_accuracy: 0.7982\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.7453 - accuracy: 0.7063 - val_loss: 0.4800 - val_accuracy: 0.7982\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7063 - accuracy: 0.7069 - val_loss: 0.4800 - val_accuracy: 0.7982\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.7343 - accuracy: 0.6988 - val_loss: 0.4799 - val_accuracy: 0.7982\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6979 - accuracy: 0.7082 - val_loss: 0.4799 - val_accuracy: 0.7982\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6746 - accuracy: 0.7036 - val_loss: 0.4799 - val_accuracy: 0.7982\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7359 - accuracy: 0.7050 - val_loss: 0.4799 - val_accuracy: 0.7982\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7187 - accuracy: 0.7030 - val_loss: 0.4798 - val_accuracy: 0.7982\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.7054 - accuracy: 0.7099 - val_loss: 0.4798 - val_accuracy: 0.7982\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7718 - accuracy: 0.7069 - val_loss: 0.4798 - val_accuracy: 0.7982\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7465 - accuracy: 0.7085 - val_loss: 0.4798 - val_accuracy: 0.7982\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7315 - accuracy: 0.7073 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6815 - accuracy: 0.7108 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.7047 - accuracy: 0.7031 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.7485 - accuracy: 0.7059 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6358 - accuracy: 0.7099 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.7303 - accuracy: 0.7071 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7047 - accuracy: 0.7122 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6599 - accuracy: 0.7113 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7731 - accuracy: 0.7063 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6520 - accuracy: 0.7198 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6221 - accuracy: 0.7153 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.7819 - accuracy: 0.7089 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7859 - accuracy: 0.7054 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6393 - accuracy: 0.7123 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.7351 - accuracy: 0.7085 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7605 - accuracy: 0.6970 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6707 - accuracy: 0.7122 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7001 - accuracy: 0.7163 - val_loss: 0.4797 - val_accuracy: 0.7982\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7491 - accuracy: 0.7031 - val_loss: 0.4796 - val_accuracy: 0.7982\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.7564 - accuracy: 0.7089 - val_loss: 0.4796 - val_accuracy: 0.7982\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.7024 - accuracy: 0.7078 - val_loss: 0.4796 - val_accuracy: 0.7982\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.6858 - accuracy: 0.7061 - val_loss: 0.4796 - val_accuracy: 0.7982\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6865 - accuracy: 0.7026 - val_loss: 0.4796 - val_accuracy: 0.7982\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7288 - accuracy: 0.7009 - val_loss: 0.4796 - val_accuracy: 0.7982\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6754 - accuracy: 0.7158 - val_loss: 0.4795 - val_accuracy: 0.7982\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6854 - accuracy: 0.7061 - val_loss: 0.4795 - val_accuracy: 0.7982\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7166 - accuracy: 0.7148 - val_loss: 0.4795 - val_accuracy: 0.7982\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.5764 - accuracy: 0.7151 - val_loss: 0.4795 - val_accuracy: 0.7982\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7035 - accuracy: 0.7148 - val_loss: 0.4794 - val_accuracy: 0.7982\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6744 - accuracy: 0.7165 - val_loss: 0.4794 - val_accuracy: 0.7982\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6617 - accuracy: 0.7066 - val_loss: 0.4794 - val_accuracy: 0.7982\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6480 - accuracy: 0.7120 - val_loss: 0.4794 - val_accuracy: 0.7982\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6454 - accuracy: 0.7071 - val_loss: 0.4793 - val_accuracy: 0.7982\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6739 - accuracy: 0.7163 - val_loss: 0.4793 - val_accuracy: 0.7982\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7191 - accuracy: 0.7111 - val_loss: 0.4793 - val_accuracy: 0.7982\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6835 - accuracy: 0.7153 - val_loss: 0.4793 - val_accuracy: 0.7982\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.7226 - accuracy: 0.7083 - val_loss: 0.4793 - val_accuracy: 0.7982\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.6465 - accuracy: 0.7128 - val_loss: 0.4793 - val_accuracy: 0.7982\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.6952 - accuracy: 0.7064 - val_loss: 0.4792 - val_accuracy: 0.7982\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7040 - accuracy: 0.7095 - val_loss: 0.4792 - val_accuracy: 0.7982\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.7926 - accuracy: 0.7002 - val_loss: 0.4792 - val_accuracy: 0.7982\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6527 - accuracy: 0.7080 - val_loss: 0.4792 - val_accuracy: 0.7982\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6855 - accuracy: 0.7130 - val_loss: 0.4791 - val_accuracy: 0.7982\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6608 - accuracy: 0.7181 - val_loss: 0.4791 - val_accuracy: 0.7982\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7031 - accuracy: 0.7030 - val_loss: 0.4791 - val_accuracy: 0.7982\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7039 - accuracy: 0.7104 - val_loss: 0.4791 - val_accuracy: 0.7982\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6374 - accuracy: 0.7116 - val_loss: 0.4790 - val_accuracy: 0.7982\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.7089 - accuracy: 0.7127 - val_loss: 0.4790 - val_accuracy: 0.7982\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6349 - accuracy: 0.7092 - val_loss: 0.4790 - val_accuracy: 0.7982\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6721 - accuracy: 0.7066 - val_loss: 0.4789 - val_accuracy: 0.7982\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6682 - accuracy: 0.7186 - val_loss: 0.4789 - val_accuracy: 0.7982\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.7409 - accuracy: 0.7075 - val_loss: 0.4789 - val_accuracy: 0.7982\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6776 - accuracy: 0.7120 - val_loss: 0.4789 - val_accuracy: 0.7982\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.6632 - accuracy: 0.7068 - val_loss: 0.4788 - val_accuracy: 0.7982\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.6769 - accuracy: 0.7144 - val_loss: 0.4788 - val_accuracy: 0.7982\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6223 - accuracy: 0.7165 - val_loss: 0.4788 - val_accuracy: 0.7982\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6623 - accuracy: 0.7234 - val_loss: 0.4787 - val_accuracy: 0.7982\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.7563 - accuracy: 0.7104 - val_loss: 0.4787 - val_accuracy: 0.7982\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.6429 - accuracy: 0.7102 - val_loss: 0.4787 - val_accuracy: 0.7982\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6691 - accuracy: 0.7099 - val_loss: 0.4787 - val_accuracy: 0.7982\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6422 - accuracy: 0.7220 - val_loss: 0.4787 - val_accuracy: 0.7982\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6532 - accuracy: 0.7101 - val_loss: 0.4786 - val_accuracy: 0.7982\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6798 - accuracy: 0.7217 - val_loss: 0.4786 - val_accuracy: 0.7982\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7004 - accuracy: 0.7125 - val_loss: 0.4786 - val_accuracy: 0.7982\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7094 - accuracy: 0.7071 - val_loss: 0.4786 - val_accuracy: 0.7982\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6909 - accuracy: 0.7123 - val_loss: 0.4786 - val_accuracy: 0.7982\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6718 - accuracy: 0.7149 - val_loss: 0.4785 - val_accuracy: 0.7982\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.6612 - accuracy: 0.7160 - val_loss: 0.4785 - val_accuracy: 0.7982\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6608 - accuracy: 0.7175 - val_loss: 0.4785 - val_accuracy: 0.7982\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.6591 - accuracy: 0.7144 - val_loss: 0.4785 - val_accuracy: 0.7982\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.6779 - accuracy: 0.7099 - val_loss: 0.4785 - val_accuracy: 0.7982\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7317 - accuracy: 0.7130 - val_loss: 0.4785 - val_accuracy: 0.7982\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.6986 - accuracy: 0.7208 - val_loss: 0.4785 - val_accuracy: 0.7982\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6594 - accuracy: 0.7151 - val_loss: 0.4784 - val_accuracy: 0.7982\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6212 - accuracy: 0.7175 - val_loss: 0.4784 - val_accuracy: 0.7982\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.7577 - accuracy: 0.7078 - val_loss: 0.4784 - val_accuracy: 0.7982\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.7380 - accuracy: 0.7115 - val_loss: 0.4784 - val_accuracy: 0.7982\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6764 - accuracy: 0.7135 - val_loss: 0.4784 - val_accuracy: 0.7982\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.5865 - accuracy: 0.7210 - val_loss: 0.4783 - val_accuracy: 0.7982\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.7150 - accuracy: 0.7179 - val_loss: 0.4783 - val_accuracy: 0.7982\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.8253 - accuracy: 0.7113 - val_loss: 0.4783 - val_accuracy: 0.7982\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.7194 - accuracy: 0.7082 - val_loss: 0.4783 - val_accuracy: 0.7982\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.7052 - accuracy: 0.7087 - val_loss: 0.4783 - val_accuracy: 0.7982\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6825 - accuracy: 0.7174 - val_loss: 0.4783 - val_accuracy: 0.7982\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7563 - accuracy: 0.7116 - val_loss: 0.4783 - val_accuracy: 0.7982\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6550 - accuracy: 0.7153 - val_loss: 0.4783 - val_accuracy: 0.7982\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.6140 - accuracy: 0.7167 - val_loss: 0.4782 - val_accuracy: 0.7982\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7627 - accuracy: 0.7175 - val_loss: 0.4782 - val_accuracy: 0.7982\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.7484 - accuracy: 0.7120 - val_loss: 0.4782 - val_accuracy: 0.7982\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6601 - accuracy: 0.7158 - val_loss: 0.4782 - val_accuracy: 0.7982\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6570 - accuracy: 0.7137 - val_loss: 0.4782 - val_accuracy: 0.7982\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7268 - accuracy: 0.7168 - val_loss: 0.4782 - val_accuracy: 0.7982\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6536 - accuracy: 0.7155 - val_loss: 0.4782 - val_accuracy: 0.7982\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6425 - accuracy: 0.7134 - val_loss: 0.4781 - val_accuracy: 0.7982\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6110 - accuracy: 0.7134 - val_loss: 0.4781 - val_accuracy: 0.7982\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.7260 - accuracy: 0.7198 - val_loss: 0.4781 - val_accuracy: 0.7982\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6503 - accuracy: 0.7127 - val_loss: 0.4781 - val_accuracy: 0.7982\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6556 - accuracy: 0.7163 - val_loss: 0.4781 - val_accuracy: 0.7982\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6332 - accuracy: 0.7186 - val_loss: 0.4780 - val_accuracy: 0.7982\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6550 - accuracy: 0.7085 - val_loss: 0.4780 - val_accuracy: 0.7982\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6308 - accuracy: 0.7130 - val_loss: 0.4780 - val_accuracy: 0.7982\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6068 - accuracy: 0.7177 - val_loss: 0.4780 - val_accuracy: 0.7982\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.7269 - accuracy: 0.7052 - val_loss: 0.4780 - val_accuracy: 0.7982\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.6481 - accuracy: 0.7149 - val_loss: 0.4779 - val_accuracy: 0.7982\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7153 - accuracy: 0.7148 - val_loss: 0.4779 - val_accuracy: 0.7982\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6727 - accuracy: 0.7156 - val_loss: 0.4779 - val_accuracy: 0.7982\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6425 - accuracy: 0.7184 - val_loss: 0.4779 - val_accuracy: 0.7982\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6301 - accuracy: 0.7196 - val_loss: 0.4779 - val_accuracy: 0.7982\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6316 - accuracy: 0.7200 - val_loss: 0.4778 - val_accuracy: 0.7982\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.7493 - accuracy: 0.7194 - val_loss: 0.4778 - val_accuracy: 0.7982\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7171 - accuracy: 0.7224 - val_loss: 0.4778 - val_accuracy: 0.7982\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.6506 - accuracy: 0.7288 - val_loss: 0.4778 - val_accuracy: 0.7982\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6973 - accuracy: 0.7130 - val_loss: 0.4778 - val_accuracy: 0.7982\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.6748 - accuracy: 0.7177 - val_loss: 0.4777 - val_accuracy: 0.7982\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6799 - accuracy: 0.7184 - val_loss: 0.4777 - val_accuracy: 0.7982\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7052 - accuracy: 0.7049 - val_loss: 0.4777 - val_accuracy: 0.7982\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.6612 - accuracy: 0.7156 - val_loss: 0.4777 - val_accuracy: 0.7982\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.6151 - accuracy: 0.7172 - val_loss: 0.4777 - val_accuracy: 0.7982\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6137 - accuracy: 0.7174 - val_loss: 0.4776 - val_accuracy: 0.7982\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.6891 - accuracy: 0.7193 - val_loss: 0.4776 - val_accuracy: 0.7982\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.7043 - accuracy: 0.7198 - val_loss: 0.4776 - val_accuracy: 0.7982\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6482 - accuracy: 0.7151 - val_loss: 0.4776 - val_accuracy: 0.7982\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6844 - accuracy: 0.7207 - val_loss: 0.4776 - val_accuracy: 0.7982\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6117 - accuracy: 0.7189 - val_loss: 0.4776 - val_accuracy: 0.7982\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6765 - accuracy: 0.7153 - val_loss: 0.4776 - val_accuracy: 0.7982\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7255 - accuracy: 0.7170 - val_loss: 0.4776 - val_accuracy: 0.7982\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6654 - accuracy: 0.7181 - val_loss: 0.4776 - val_accuracy: 0.7982\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6376 - accuracy: 0.7179 - val_loss: 0.4775 - val_accuracy: 0.7982\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6149 - accuracy: 0.7165 - val_loss: 0.4775 - val_accuracy: 0.7982\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6348 - accuracy: 0.7179 - val_loss: 0.4775 - val_accuracy: 0.7982\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6995 - accuracy: 0.7085 - val_loss: 0.4775 - val_accuracy: 0.7982\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6165 - accuracy: 0.7252 - val_loss: 0.4775 - val_accuracy: 0.7982\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6621 - accuracy: 0.7186 - val_loss: 0.4775 - val_accuracy: 0.7982\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7360 - accuracy: 0.7172 - val_loss: 0.4775 - val_accuracy: 0.7982\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.7464 - accuracy: 0.7142 - val_loss: 0.4774 - val_accuracy: 0.7982\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.6711 - accuracy: 0.7175 - val_loss: 0.4774 - val_accuracy: 0.7982\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.6372 - accuracy: 0.7240 - val_loss: 0.4774 - val_accuracy: 0.7982\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.6443 - accuracy: 0.7153 - val_loss: 0.4774 - val_accuracy: 0.7982\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6829 - accuracy: 0.7144 - val_loss: 0.4774 - val_accuracy: 0.7982\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.6705 - accuracy: 0.7116 - val_loss: 0.4773 - val_accuracy: 0.7982\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6340 - accuracy: 0.7194 - val_loss: 0.4773 - val_accuracy: 0.7982\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7074 - accuracy: 0.7038 - val_loss: 0.4773 - val_accuracy: 0.7982\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7043 - accuracy: 0.7205 - val_loss: 0.4772 - val_accuracy: 0.7982\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7045 - accuracy: 0.7170 - val_loss: 0.4772 - val_accuracy: 0.7982\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6547 - accuracy: 0.7158 - val_loss: 0.4772 - val_accuracy: 0.7982\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6341 - accuracy: 0.7141 - val_loss: 0.4772 - val_accuracy: 0.7982\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6654 - accuracy: 0.7153 - val_loss: 0.4772 - val_accuracy: 0.7982\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.5744 - accuracy: 0.7248 - val_loss: 0.4771 - val_accuracy: 0.7982\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7235 - accuracy: 0.7099 - val_loss: 0.4771 - val_accuracy: 0.7982\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6967 - accuracy: 0.7130 - val_loss: 0.4771 - val_accuracy: 0.7982\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.6753 - accuracy: 0.7208 - val_loss: 0.4771 - val_accuracy: 0.7982\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6132 - accuracy: 0.7120 - val_loss: 0.4770 - val_accuracy: 0.7982\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.7086 - accuracy: 0.7125 - val_loss: 0.4770 - val_accuracy: 0.7982\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6136 - accuracy: 0.7158 - val_loss: 0.4770 - val_accuracy: 0.7982\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.6654 - accuracy: 0.7222 - val_loss: 0.4769 - val_accuracy: 0.7982\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6645 - accuracy: 0.7193 - val_loss: 0.4769 - val_accuracy: 0.7982\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6020 - accuracy: 0.7181 - val_loss: 0.4769 - val_accuracy: 0.7982\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6946 - accuracy: 0.7215 - val_loss: 0.4769 - val_accuracy: 0.7982\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7054 - accuracy: 0.7158 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.7777 - accuracy: 0.7172 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6838 - accuracy: 0.7292 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.6314 - accuracy: 0.7271 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6638 - accuracy: 0.7271 - val_loss: 0.4767 - val_accuracy: 0.7982\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6238 - accuracy: 0.7274 - val_loss: 0.4767 - val_accuracy: 0.7982\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6948 - accuracy: 0.7161 - val_loss: 0.4767 - val_accuracy: 0.7982\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6231 - accuracy: 0.7170 - val_loss: 0.4767 - val_accuracy: 0.7982\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.6162 - accuracy: 0.7170 - val_loss: 0.4767 - val_accuracy: 0.7982\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6417 - accuracy: 0.7226 - val_loss: 0.4767 - val_accuracy: 0.7982\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7250 - accuracy: 0.7163 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6729 - accuracy: 0.7134 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6629 - accuracy: 0.7125 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6960 - accuracy: 0.7236 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6483 - accuracy: 0.7217 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7366 - accuracy: 0.7153 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6308 - accuracy: 0.7229 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6919 - accuracy: 0.7189 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6366 - accuracy: 0.7203 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.5996 - accuracy: 0.7158 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6925 - accuracy: 0.7191 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6643 - accuracy: 0.7236 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7306 - accuracy: 0.7168 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6739 - accuracy: 0.7128 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6676 - accuracy: 0.7151 - val_loss: 0.4768 - val_accuracy: 0.7982\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.6629 - accuracy: 0.7151 - val_loss: 0.4767 - val_accuracy: 0.7982\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6883 - accuracy: 0.7262 - val_loss: 0.4767 - val_accuracy: 0.7982\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6255 - accuracy: 0.7226 - val_loss: 0.4767 - val_accuracy: 0.7982\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6177 - accuracy: 0.7149 - val_loss: 0.4767 - val_accuracy: 0.7982\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.5894 - accuracy: 0.7248 - val_loss: 0.4767 - val_accuracy: 0.7982\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6175 - accuracy: 0.7160 - val_loss: 0.4767 - val_accuracy: 0.7982\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6167 - accuracy: 0.7194 - val_loss: 0.4766 - val_accuracy: 0.7982\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7152 - accuracy: 0.7151 - val_loss: 0.4766 - val_accuracy: 0.7982\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.5285 - accuracy: 0.7286 - val_loss: 0.4766 - val_accuracy: 0.7982\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6825 - accuracy: 0.7174 - val_loss: 0.4766 - val_accuracy: 0.7982\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6676 - accuracy: 0.7200 - val_loss: 0.4766 - val_accuracy: 0.7982\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6132 - accuracy: 0.7306 - val_loss: 0.4765 - val_accuracy: 0.7982\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6448 - accuracy: 0.7188 - val_loss: 0.4765 - val_accuracy: 0.7982\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.7099 - accuracy: 0.7205 - val_loss: 0.4765 - val_accuracy: 0.7982\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.6567 - accuracy: 0.7191 - val_loss: 0.4765 - val_accuracy: 0.7982\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6108 - accuracy: 0.7127 - val_loss: 0.4765 - val_accuracy: 0.7982\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.7083 - accuracy: 0.7134 - val_loss: 0.4764 - val_accuracy: 0.7982\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.7001 - accuracy: 0.7260 - val_loss: 0.4764 - val_accuracy: 0.7982\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.7324 - accuracy: 0.7158 - val_loss: 0.4764 - val_accuracy: 0.7982\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.7129 - accuracy: 0.7167 - val_loss: 0.4764 - val_accuracy: 0.7982\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6727 - accuracy: 0.7188 - val_loss: 0.4764 - val_accuracy: 0.7982\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6118 - accuracy: 0.7253 - val_loss: 0.4763 - val_accuracy: 0.7982\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.6435 - accuracy: 0.7252 - val_loss: 0.4763 - val_accuracy: 0.7982\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.7058 - accuracy: 0.7194 - val_loss: 0.4763 - val_accuracy: 0.7982\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6607 - accuracy: 0.7155 - val_loss: 0.4763 - val_accuracy: 0.7982\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.7105 - accuracy: 0.7219 - val_loss: 0.4763 - val_accuracy: 0.7982\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.6336 - accuracy: 0.7186 - val_loss: 0.4763 - val_accuracy: 0.7982\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6464 - accuracy: 0.7142 - val_loss: 0.4763 - val_accuracy: 0.7982\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.7554 - accuracy: 0.7167 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.6540 - accuracy: 0.7267 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.7244 - accuracy: 0.7149 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.6896 - accuracy: 0.7155 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.6909 - accuracy: 0.7229 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.6294 - accuracy: 0.7267 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.5513 - accuracy: 0.7257 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.5662 - accuracy: 0.7259 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6628 - accuracy: 0.7273 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.5976 - accuracy: 0.7189 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6853 - accuracy: 0.7207 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6637 - accuracy: 0.7158 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6092 - accuracy: 0.7198 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6362 - accuracy: 0.7248 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.6496 - accuracy: 0.7168 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6392 - accuracy: 0.7160 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6824 - accuracy: 0.7219 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6663 - accuracy: 0.7200 - val_loss: 0.4762 - val_accuracy: 0.7982\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6959 - accuracy: 0.7158 - val_loss: 0.4761 - val_accuracy: 0.7982\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.6455 - accuracy: 0.7132 - val_loss: 0.4761 - val_accuracy: 0.7982\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6751 - accuracy: 0.7148 - val_loss: 0.4761 - val_accuracy: 0.7982\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7021 - accuracy: 0.7158 - val_loss: 0.4761 - val_accuracy: 0.7982\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6530 - accuracy: 0.7201 - val_loss: 0.4761 - val_accuracy: 0.7982\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6702 - accuracy: 0.7212 - val_loss: 0.4761 - val_accuracy: 0.7982\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.5855 - accuracy: 0.7264 - val_loss: 0.4760 - val_accuracy: 0.7982\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6389 - accuracy: 0.7238 - val_loss: 0.4760 - val_accuracy: 0.7982\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6796 - accuracy: 0.7267 - val_loss: 0.4760 - val_accuracy: 0.7982\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6359 - accuracy: 0.7222 - val_loss: 0.4760 - val_accuracy: 0.7982\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7022 - accuracy: 0.7257 - val_loss: 0.4760 - val_accuracy: 0.7982\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6558 - accuracy: 0.7269 - val_loss: 0.4760 - val_accuracy: 0.7982\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.6959 - accuracy: 0.7250 - val_loss: 0.4760 - val_accuracy: 0.7982\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6598 - accuracy: 0.7274 - val_loss: 0.4759 - val_accuracy: 0.7982\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6764 - accuracy: 0.7307 - val_loss: 0.4759 - val_accuracy: 0.7982\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7161 - accuracy: 0.7304 - val_loss: 0.4759 - val_accuracy: 0.7982\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.5580 - accuracy: 0.7370 - val_loss: 0.4759 - val_accuracy: 0.7982\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.5540 - accuracy: 0.7262 - val_loss: 0.4759 - val_accuracy: 0.7982\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.7439 - accuracy: 0.7186 - val_loss: 0.4759 - val_accuracy: 0.7982\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6281 - accuracy: 0.7257 - val_loss: 0.4759 - val_accuracy: 0.7982\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6844 - accuracy: 0.7288 - val_loss: 0.4758 - val_accuracy: 0.7982\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.5955 - accuracy: 0.7220 - val_loss: 0.4758 - val_accuracy: 0.7982\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6600 - accuracy: 0.7309 - val_loss: 0.4758 - val_accuracy: 0.7982\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7016 - accuracy: 0.7309 - val_loss: 0.4758 - val_accuracy: 0.7982\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7105 - accuracy: 0.7276 - val_loss: 0.4758 - val_accuracy: 0.7982\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6171 - accuracy: 0.7250 - val_loss: 0.4757 - val_accuracy: 0.7982\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.6124 - accuracy: 0.7245 - val_loss: 0.4757 - val_accuracy: 0.7982\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6911 - accuracy: 0.7274 - val_loss: 0.4757 - val_accuracy: 0.7982\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6500 - accuracy: 0.7292 - val_loss: 0.4757 - val_accuracy: 0.7982\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6845 - accuracy: 0.7304 - val_loss: 0.4756 - val_accuracy: 0.7982\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6707 - accuracy: 0.7273 - val_loss: 0.4756 - val_accuracy: 0.7982\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6426 - accuracy: 0.7252 - val_loss: 0.4756 - val_accuracy: 0.7982\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7049 - accuracy: 0.7280 - val_loss: 0.4756 - val_accuracy: 0.7982\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6299 - accuracy: 0.7293 - val_loss: 0.4756 - val_accuracy: 0.7982\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.5965 - accuracy: 0.7295 - val_loss: 0.4756 - val_accuracy: 0.7982\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.5863 - accuracy: 0.7351 - val_loss: 0.4756 - val_accuracy: 0.7982\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6302 - accuracy: 0.7280 - val_loss: 0.4755 - val_accuracy: 0.7982\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6411 - accuracy: 0.7323 - val_loss: 0.4755 - val_accuracy: 0.7982\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6407 - accuracy: 0.7260 - val_loss: 0.4755 - val_accuracy: 0.7982\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6260 - accuracy: 0.7333 - val_loss: 0.4755 - val_accuracy: 0.7982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "vBvO4S4GWjYl",
        "outputId": "f7a13f31-d2ab-493a-9046-07dafd793820"
      },
      "source": [
        "# list all data in history\n",
        "\n",
        "print(model_history.history.keys())\n",
        "# summarize history for accuracy\n",
        "\n",
        "plt.plot(model_history.history['acc'])\n",
        "plt.plot(model_history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-815ba8751159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'acc'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "z495R0V3WvmV",
        "outputId": "fabb3fd2-9c31-494a-fb6f-5d8261f04b9b"
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(model_history.history['loss'])\n",
        "plt.plot(model_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS-30HxKWvp_",
        "outputId": "a57dbc43-72a0-4b8e-9c4e-f4ab0f25e09b"
      },
      "source": [
        "# Part 3 - Making the predictions and evaluating the model\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "y_pred"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[False],\n",
              "       [False],\n",
              "       [False],\n",
              "       ...,\n",
              "       [False],\n",
              "       [False],\n",
              "       [False]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOrZR6cvWJsi",
        "outputId": "a9c0dece-c05c-4191-d0ed-b837ecb8f00b"
      },
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1595,    0],\n",
              "       [ 405,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxk2No0TW29d",
        "outputId": "c71ce217-aac1-443d-c150-cfbe8eb949b1"
      },
      "source": [
        "# Calculate the Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "score=accuracy_score(y_pred,y_test)\n",
        "score"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7975"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx8ZZhNYh3c0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}