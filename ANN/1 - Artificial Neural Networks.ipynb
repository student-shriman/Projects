{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What is Deep Learning ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To understand what Deep Learning is, we first need to understand the relationship that Deep learning has with Machine learning,\n",
    "     Neural Networks, and Artificial Intelligence.\n",
    " \n",
    " The best way to think of this relationship is to visualize them as concentric circles.\n",
    "\n",
    "<img src='images/ai.png' width='300px'>\n",
    "\n",
    "\n",
    "  At the outer most ring you have Artificial Intelligence.\n",
    "    Second layer inside of Artificial Intelligence is Machine learning and Deep learning is inside the Machine learning at the centre.\n",
    "    \n",
    "    Broadly speaking, Deep Learning is a more approachable name for an Artificial Neural Network. \n",
    "    The “Deep” in Deep Learning refers to the depth of the network. An Artificial Neural Network can be very shallow.\n",
    "\n",
    "Neural networks are inspired by the structure of the cerebral cortex.\n",
    "     At the basic level is the perceptron, the mathematical representation of a biological neuron. \n",
    "         Like in the cerebral cortex, there can be several layers of interconnected perceptrons.\n",
    "\n",
    "\n",
    " The first layer is the input layer. Each node in this layer takes an input, and then passes its output as the input\n",
    "        to each node in the next layer. There are generally no connections between nodes in the same layer and \n",
    "             the last layer produces the outputs.\n",
    "\n",
    "\n",
    " We call the middle part the hidden layer. These neurons have no connection to the outside (e.g. input or output) and \n",
    "   are only activated by nodes in the previous layer.\n",
    "\n",
    "<img src='images/nn-diagram.png' width='540 px'>\n",
    "\n",
    "Think of Deep Learning as the technique for learning in Neural networks that utilizes multiple layers of abstraction to solve pattern recognition problems. In the 1980s, most neural networks were a single layer due to the cost of computation and availability of data.\n",
    "\n",
    "Machine learning is considered a branch or approach of Artificial intelligence, whereas Deep Learning is a specialized type of Machine Learning.\n",
    "\n",
    "\n",
    "Machine learning involves Computer's intelligence that doesn’t know the answers up front. Instead, the program will run against training data, verify the success of its attempts, and modify its approach accordingly.\n",
    "   \n",
    "Machine learning typical requires a sophisticated education, spanning software engineering and computer science\n",
    "     to statistical methods and linear algebra.\n",
    "\n",
    "\n",
    "There are two broad classes of machine learning methods:\n",
    "\n",
    "   * Supervised learning\n",
    "   * Unsupervised learning\n",
    "\n",
    "\n",
    "In supervised learning, a machine learning algorithm uses a labeled dataset to infer the desired outcome. This takes a lot of data and time, since the data needs to be labeled by hand. Supervised learning is great for classification and regression problems.\n",
    "\n",
    "For example, let’s say that we were running a company and want to determine the effect of bonuses on employee retention. If we had historical data – i.e. employee bonus amount and tenure – we could use supervised machine learning.\n",
    "\n",
    "With unsupervised learning, there aren’t any predefined or corresponding answers. The goal is to figure out the hidden patterns in the data. It’s usually used for clustering and associative tasks, like grouping customers by behavior. Amazon’s “customers who also bought…” recommendations are a type of associative task.\n",
    "\n",
    "While supervised learning can be useful, we often have to resort to unsupervised learning. Deep learning has proven to be an effective unsupervised learning technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is Deep Learning Important?\n",
    "\n",
    "Computers have long had techniques for recognizing features inside of images. The results weren’t always great. Computer vision has been a main beneficiary of Deep learning. Computer Vision using Deep Learning now rivals humans on many image recognition tasks.\n",
    "\n",
    "\n",
    "Facebook has had great success with identifying faces in photographs by using deep learning. It’s not just a marginal improvement, but a game changer: “Asked whether two unfamiliar photos of faces show the same person, a human being will get it right 97.53 percent of the time. New software developed by researchers at Facebook can score 97.25 percent on the same challenge, regardless of variations in lighting or whether the person in the picture is directly facing the camera.”\n",
    "\n",
    "\n",
    "Speech recognition is a another area that’s felt deep learning’s impact. Spoken languages are so vast and ambiguous. Baidu – one of the leading search engines of China – has developed a voice recognition system that is faster and more accurate than humans at producing text on a mobile phone. In both English and Mandarin.\n",
    "\n",
    "\n",
    "What is particularly fascinating, is that generalizing the two languages didn’t require much additional design effort: “Historically, people viewed Chinese and English as two vastly different languages, and so there was a need to design very different features,” Andrew Ng says, chief scientist at Baidu. “The learning algorithms are now so general that you can just learn.”\n",
    "\n",
    "Google is now using deep learning to manage the energy at the company’s data centers. They’ve cut their energy needs for cooling by 40%. That translates to about a 15% improvement in power usage efficiency for the company and hundreds of millions of dollars in savings.\n",
    "\n",
    "<img src='images/andrew-ng-chief-scientist-at-baidu-30-638.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Biological Neurons\n",
    "\n",
    "Biological Neuron, BNN is an unusual-looking cell mostly found in animal cerebral cortexes (e.g., your brain), composed of a cell body containing the nucleus and most of the cell’s complex components, and many branching extensions called Dendrites plus one very long extension called the Axon.\n",
    "\n",
    "The Axon’s length may be just a few times longer than the cell body, or up to tens of thousands of times longer. Near its extremity the axon splits off into many branches called Telodendria, and at the tip of these branches are minuscule structures called Synaptic Terminals (or simply Synapses), which are connected to the dendrites (or directly to the cell body) of other neurons. \n",
    "\n",
    "Biological neurons receive short electrical impulses called Signals from other neurons via these Synapses.\n",
    "When a neuron receives a sufficient number of signals from other neurons within a few milliseconds, it fires its own signals.\n",
    "\n",
    "Thus, individual Biological Neurons seem to behave in a rather simple way, but they are organized in a vast network of billions of neurons, each neuron typically connected to thousands of other neurons.\n",
    "\n",
    "Highly complex computations can be performed by a vast network of fairly simple neurons, much like a complex ant hill can emerge from the combined efforts of simple ants. \n",
    "\n",
    "The architecture of Bilogical Neural Network (BNN) is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers.\n",
    "\n",
    "<img src='images/bnn.jpg'>\n",
    "<img src='images/human bnn.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Diagram of an Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  There are 3 different types of layers lies inside a Neural Networks.\n",
    "    \n",
    "        * Input Layer    *  Hidden Layers    *  Output Layer\n",
    "    \n",
    " =>  In Input Layer, Number of Perceptrons depends upon the total number of Features available in dataset.\n",
    "    \n",
    " =>  Perceptrons of Input Layer receives the inputted data. In above case, Row 1 is going to be inputted in \n",
    "      Input Layer, then 2nd Row, 3rd Row and so on .\n",
    "    \n",
    " =>  The main function of Input layer is just only getting the inputs from dataset.\n",
    "\n",
    " =>  This Inputted data goes to 2nd Layer of Neural networks (Hidden layer) in such a way that Data from each \n",
    "      Perceptron of Input layer goes inputted to each and every Perceptrons of Hidden layer.\n",
    "\n",
    " =>  When we try to pass our data from Input layer to Hidden layer, We multiply some randomly selected weights,\n",
    "        W1 to inputs X1 and then add some randomly selected Bias, B1 to the inputs. \n",
    "      Then this Linear mix of inputs (W1.X1 + B1) get entered into each Perceptrons of Hidden layer.\n",
    "        \n",
    " =>  Weights and Bias are learnable parameters and they are added to get patterens by making relations \n",
    "       in dataset more easily.\n",
    "\n",
    " =>  If we have only 2 Layers i.e. Input Layer and Output Layer, Whatever data we passes to Input Layer will\n",
    "      be outputted based on Output Functions and with no parameters it is very difficult to find the \n",
    "       relationships between the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/nn.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feed - Forward Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ##############   #############   FFN in Training of Neural networks   ################   ##############\n",
    "    \n",
    " =>  Training of an Artificial Neural Networks has been classified into two diferent processes.\n",
    "       \n",
    "         * Feed Forward Connection (FFN)       &          * Backward Propagation\n",
    "        \n",
    " =>  In Feed Forward Connections, Data is getting inputted into the Input Layer of Neural Network and then \n",
    "       it get multiplied and added with some randomly selected parameters like as 'weights' and 'Bias'.\n",
    "     \n",
    " =>  Weights & Bias are some Learnable parameters and they are getting introduced into the Networks for\n",
    "       getting an ease of Acces in finding  Relations between data.\n",
    "    \n",
    " =>  After introducing Weights and Bias, a Linear mix of Inputs are getting inputted to the Perceptron of\n",
    "       next layer of Network and there will be similar inputs from all other perceptrons of Input Layer.\n",
    "     So a Summation of all Linear Mix of Inputs will be inputted to each perceptrons of Hidden Layer.\n",
    "    \n",
    " =>  Inside a Hidden layer, a Linear mix of data enters as Input, so we need some particular function which \n",
    "      will be able to Filter, Restrict, Normalize and Non-Linearize the Inputted dataset, which is passed \n",
    "       from Input layer to Hidden layer. ThereFore Activation Function comes into picture. \n",
    "        We can add as many Hidden layers but in a certain limit.\n",
    "        \n",
    " =>  Activation functions are kind of functions, which always helps Perceptrons to fire itself in certain\n",
    "      direction or in a certain range and also try to restricts some sort of dataset.\n",
    "    \n",
    "     Examples of Activation function - \n",
    "\n",
    "        *  Sigmoid Activation Function         *  Tanh Activation Function  \n",
    "        *  RelU Activation Function            *  Leaky RelU Activation function etc.\n",
    "        \n",
    " =>  We can use a Single Activation Function inside a Hidden Layer.\n",
    "      Activation function does some particular operations and give some outputs (say o1, o2), which get \n",
    "        multiplied with some different weights again and then enters into Next Layer.\n",
    "        \n",
    "        \n",
    " =>  In Output Layer, We again uses a Function which is known as Output Function. \n",
    "    Here all Inputs are get added together by using Output function and then we get a final output or\n",
    "      prediction. \n",
    " \n",
    " =>  Output Functions are similar to Activation functions but it always getting used in the output Layer of\n",
    "       the Network.\n",
    "    \n",
    " =>  There will be some differences between the Expected and Predicted outputs, and this difference is\n",
    "      termed as Loss or cost. \n",
    "        \n",
    " =>  This whole process is known as Feed Forward Connection or Feed Forward Networks.\n",
    "      In FFN, our Network does not try to learn something. Learning Never happens in FFN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Assigning Weights and Bias To Inputs   \n",
    "\n",
    "<img src='images/weight.png' width='720px'>\n",
    "   \n",
    "    \n",
    "##   Input of Perceptrons in Hidden layer   \n",
    "\n",
    "<img src='images/total-input.jpg' width='480px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  In FFN, we get some Losses due to introducing some randomly selected parameters and our whole objective\n",
    "      and idea is to reduce this Loss. For reducing this loss, Backward Propagation comes into picture.\n",
    "    \n",
    " =>  Backward Propagation is a method for efficiently computing the gradient of the cost function of a \n",
    "      Neural network with respect to its parameters. These partial derivatives can then be used to update the \n",
    "        parameters using 'Gradient Descent'.\n",
    "\n",
    " =>  The gradient shows how much the parameters 'weights' & 'Biases' needs to change (in positive or negative \n",
    "        direction) to minimize the Cost function 'C' .\n",
    "    \n",
    " =>  In Backward Propagation, it will try to update the randomly selected parameters by using some \n",
    "      'optimization' mechanism and it always occurs in backward direction and updates the parameters \n",
    "     layer-by-layer in Network on the basis of losses.\n",
    "      For updating the parameters, it uses a Differential equation.\n",
    "        \n",
    " =>  With updated parameters, it will start the Feed Forward Connection again and calculates some Losses\n",
    "      and then it again starts the Backward Propagation to reduce the weights. \n",
    "    It will keep updating the weights untill we get an optimal or reduced Loss and we get some pattern in\n",
    "      dataset.\n",
    "        \n",
    " =>  One Feed Forward connection and one Backward Propagation togetherly makes a cycle, known as one Epoch.\n",
    "\n",
    " =>  In Differential Eqns to update the parameters, we uses a Learning Rate to control the learning process. \n",
    "      Learning Rate should not be too high or too low, it will be in range of (0.001 to 10).\n",
    "        \n",
    " =>  At beginning, We dont have any idea about any relation or patterns between the dataset, so we introduces \n",
    "     some Learnable Parameters and in FFN, a Linear mix of Inputs goes to each and every units of \n",
    "      the Hidden Layer. In Backward Propagation, We updates the  parameters on the basis of Loss.\n",
    "        \n",
    " =>  So with updated parameters, We repeat the FFN and this time Our network drops some data, which are not\n",
    "       contributing in the result and this way, it learns the pattern between the dataset.\n",
    "    \n",
    " =>  Increment of Learnable Parameters makes our Network more flexible to learn the relationship b/w dataset.\n",
    "     Introducing more units in Hidden Layers & more Hidden Layers will give a better result but they are\n",
    "      just experimental ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Chain Rule for updating the parameters\n",
    "\n",
    "> The <b> Chain rule </b> is a way to compute the derivative of a function whose variables are themselves functions of other variables.  If $C$ is a scalar-valued function of a scalar $z$ and $z$ is itself a scalar-valued function of another scalar variable $w$, then the chain rule states that -\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial z}\\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "\n",
    "> For scalar-valued functions of more than one variable, the chain rule essentially becomes additive.  In other words, if $C$ is a scalar-valued function of $N$ variables $z_1, \\ldots, z_N$, each of which is a function of some variable $w$, the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\sum_{i = 1}^N \\frac{\\partial C}{\\partial z_i}\\frac{\\partial z_i}{\\partial w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/eqn.jpg' width = '420px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "<b> In the following derivation, we'll use the following notations. </b>\n",
    "\n",
    "$L$ - Number of layers in the network.\n",
    "\n",
    "$N^n$ - Dimensionality of layer $n \\in \\{0, \\ldots, L\\}$.  $N^0$ is the dimensionality of the input; $N^L$ is the dimensionality of the output.\n",
    "\n",
    "$W^m \\in \\mathbb{R}^{N^m \\times N^{m - 1}}$ - Weight matrix for layer $m \\in \\{1, \\ldots, L\\}$.  $W^m_{ij}$ is the weight between the $i^{th}$ unit in layer $m$ and the $j^{th}$ unit in layer $m - 1$.\n",
    "\n",
    "$b^m \\in \\mathbb{R}^{N^m}$ - Bias vector for layer $m$.\n",
    "\n",
    "$\\sigma^m$ - Nonlinear activation function of the units in layer $m$, applied elementwise.\n",
    "\n",
    "$z^m \\in \\mathbb{R}^{N^m}$ - Linear mix of the inputs to layer $m$, computed by $z^m = W^m a^{m - 1} + b^m$.\n",
    "\n",
    "$a^m \\in \\mathbb{R}^{N^m}$ - Activation of units in layer $m$, computed by $a^m = \\sigma^m(h^m) = \\sigma^m(W^m a^{m - 1} + b^m)$.  $a^L$ is the output of the network.  We define the special case $a^0$ as the input of the network.\n",
    "\n",
    "$y \\in \\mathbb{R}^{N^L}$ - Target output of the network.\n",
    "\n",
    "$C$ - Cost/error function of the network, which is a function of $a^L$ (the network output) and $y$ (treated as a constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/Ann.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b> Visit this link for a better understanding about how this back propagation occurs and updates the paramaeters throuhout the layers of a Neural network. </b>\n",
    "[Neural Networks Training Blog](https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in general\n",
    "\n",
    "> In order to train the network using a gradient descent algorithm, we need to know the gradient of each of the parameters with respect to the cost/error function $C$; that is, we need to know $\\frac{\\partial C}{\\partial W^m}$ and $\\frac{\\partial C}{\\partial b^m}$.  \n",
    "\n",
    "> It will be sufficient to derive an expression for these gradients in terms of the following terms, which we can compute based on the neural network's architecture:\n",
    "> - $\\frac{\\partial C}{\\partial a^L}$: The derivative of the cost function with respect to its argument, the output of the network\n",
    "> - $\\frac{\\partial a^m}{\\partial z^m}$: The derivative of the nonlinearity used in layer $m$ with respect to its argument\n",
    "\n",
    "> To compute the gradient of our cost/error function $C$ to $W^m_{ij}$ (a single entry in the weight matrix of the layer $m$), we can first note that $C$ is a function of $a^L$, which is itself a function of the linear mix variables $z^m_k$, which are themselves functions of the weight matrices $W^m$ and biases $b^m$.  With this in mind, we can use the chain rule as follows:\n",
    "$$\\frac{\\partial C}{\\partial W^m_{ij}} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial W^m_{ij}}$$\n",
    "\n",
    "> Note that by definition \n",
    "$$\n",
    "z^m_k = \\sum_{l = 1}^{N^m} W^m_{kl} a_l^{m - 1} + b^m_k\n",
    "$$\n",
    "It follows that $\\frac{\\partial z^m_k}{\\partial W^m_{ij}}$ will evaluate to zero when $i \\ne k$ because $z^m_k$ does not interact with any elements in $W^m$ except for those in the $k$<sup>th</sup> row, and we are only considering the entry $W^m_{ij}$.  When $i = k$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z^m_i}{\\partial W^m_{ij}} &= \\frac{\\partial}{\\partial W^m_{ij}}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right)\\\\\n",
    "&= a^{m - 1}_j\\\\\n",
    "\\rightarrow \\frac{\\partial z^m_k}{\\partial W^m_{ij}} &= \\begin{cases}\n",
    "0 & k \\ne i\\\\\n",
    "a^{m - 1}_j & k = i\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "> The fact that $\\frac{\\partial C}{\\partial a^m_k}$ is $0$ unless $k = i$ causes the summation above to collapse, giving\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m_{ij}} = \\frac{\\partial C}{\\partial z^m_i} a^{m - 1}_j$$\n",
    "\n",
    "> or in vector form\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m} a^{m - 1 \\top}$$\n",
    "\n",
    "> Similarly for the bias variables $b^m$, we have\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m_i} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial b^m_i}$$\n",
    "\n",
    "> As above, it follows that $\\frac{\\partial z^m_k}{\\partial b^m_i}$ will evaluate to zero when $i \\ne k$ because $z^m_k$ does not interact with any element in $b^m$ except $b^m_k$.  When $i = k$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z^m_i}{\\partial b^m_i} &= \\frac{\\partial}{\\partial b^m_i}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right)\\\\\n",
    "&= 1\\\\\n",
    "\\rightarrow \\frac{\\partial z^m_i}{\\partial b^m_i} &= \\begin{cases}\n",
    "0 & k \\ne i\\\\\n",
    "1 & k = i\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "> The summation also collapses to give $$\\frac{\\partial C}{\\partial b^m_i} = \\frac{\\partial C}{\\partial z^m_i}$$\n",
    "\n",
    "> or in vector form $$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n",
    "\n",
    "> Now, we must compute $\\frac{\\partial C}{\\partial z^m_k}$.  For the final layer ($m = L$), this term is straightforward to compute using the chain rule:\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^L_k} = \\frac{\\partial C}{\\partial a^L_k} \\frac{\\partial a^L_k}{\\partial z^L_k}\n",
    "$$\n",
    "\n",
    "> or, in vector form\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}\n",
    "$$\n",
    "\n",
    "> The first term $\\frac{\\partial C}{\\partial a^L}$ is just the derivative of the cost function with respect to its argument, whose form depends on the cost function chosen.  Similarly, $\\frac{\\partial a^m}{\\partial z^m}$ (for any layer $m$ includling $L$) is the derivative of the layer's nonlinearity with respect to its argument and will depend on the choice of nonlinearity.  For other layers, we again invoke the chain rule:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^m_k} &= \\frac{\\partial C}{\\partial a^m_k} \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l}\\frac{\\partial z^{m + 1}_l}{\\partial a^m_k}\\right)\\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l}\\frac{\\partial}{\\partial a^m_k} \\left(\\sum_{h = 1}^{N^m} W^{m + 1}_{lh} a_h^m + b_l^{m + 1}\\right)\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l} W^{m + 1}_{lk}\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}W^{m + 1\\top}_{kl} \\frac{\\partial C}{\\partial z^{m + 1}_l}\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "> where the last simplification was made because by convention $\\frac{\\partial C}{\\partial z^{m + 1}_l}$ is a column vector, allowing us to write the following vector form:\n",
    "$$\\frac{\\partial C}{\\partial z^m} = \\left(W^{m + 1\\top} \\frac{\\partial C}{\\partial z^{m + 1}}\\right) \\circ \\frac{\\partial a^m}{\\partial z^m}$$\n",
    "\n",
    "> Note that we now have the ingredients to efficiently compute the gradient of the cost function with respect to the network's parameters:  First, we compute $\\frac{\\partial C}{\\partial z^L_k}$ based on the choice of cost function and nonlinearity.  Then, we recursively can compute $\\frac{\\partial C}{\\partial z^m}$ layer-by-layer based on the term $\\frac{\\partial C}{\\partial z^{m + 1}}$ computed from the previous layer and the nonlinearity of the layer (this is called the \"backward pass\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Gradient Descent </b>\n",
    "\n",
    "> The simplest algorithm for iterative minimization of differentiable functions is known as just **gradient descent**.\n",
    "Recall that the gradient of a function is defined as the vector of partial derivatives:\n",
    "$$\\nabla f(x) =  [{\\partial{f}{x_1}, \\partial{f}{x_2}, \\ldots, \\partial{f}{x_n}}]$$\n",
    "and that the gradient of a function always points towards the direction of maximal increase at that point.\n",
    "\n",
    "> Equivalently, it points *away* from the direction of maximum decrease thus, if we start at any point, and keep moving in the direction of the negative gradient, we will eventually reach a local minimum.\n",
    "\n",
    "> This simple insight leads to the Gradient Descent algorithm. Outlined algorithmically, it looks like this:\n",
    "> 1. Pick a point $x_0$ as your initial guess.\n",
    "> 2. Compute the gradient at your current guess:\n",
    "> $v_i = \\nabla f(x_i)$\n",
    "> 3. Move by $\\alpha$ (your step size) in the direction of that gradient:\n",
    "> $x_{i+1} = x_i + \\alpha v_i$\n",
    "> 4. Repeat steps 1-3 until your function is close enough to zero (until $f(x_i) < \\varepsilon$ for some small tolerance $\\varepsilon$)\n",
    "\n",
    "> Note that the step size, $\\alpha$, is simply a parameter of the algorithm and has to be fixed in advance. \n",
    "![gd](http://ludovicarnold.altervista.org/wp-content/uploads/2015/01/gradient-trajectory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that the hyperbolic tangent function asymptotes at -1 and 1, rather than 0 and 1, which is sometimes beneficial, and its derivative is simple:\n",
    "> $$\\frac{d \\tanh(x)}{dx} = 1 - \\tanh^2(x)$$\n",
    "\n",
    "> Performing gradient descent will allow us to change the weights in the direction that optimially reduces the error. The next trick will be to employ the **chain rule** to decompose how the error changes as a function of the input weights into the change in error as a function of changes in the inputs to the weights, mutliplied by the changes in input values as a function of changes in the weights. \n",
    "> $$\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial h}\\frac{\\partial h}{\\partial w}$$\n",
    "\n",
    "> This will allow us to write a function describing the activations of the output weights as a function of the activations of the hidden layer nodes and the output weights, which will allow us to propagate error backwards through the network.\n",
    "\n",
    "> The second term in the chain rule simplifies to:\n",
    ">$$\\begin{align}\n",
    "\\frac{\\partial h_k}{\\partial w_{jk}} &= \\frac{\\partial \\sum_l w_{lk} a_l}{\\partial w_{jk}}  \\\\\n",
    "&= \\sum_l \\frac{\\partial w_{lk} a_l}{\\partial w_{jk}} \\\\\n",
    "& = a_j\n",
    "\\end{align}$$\n",
    "where $a_j$ is the activation of the jth hidden layer neuron.\n",
    "\n",
    "> For the first term in the chain rule above, we decompose it as well:\n",
    "$$\\frac{\\partial E}{\\partial h_k} = \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial h_k} = \\frac{\\partial E}{\\partial g(h_k)}\\frac{\\partial g(h_k)}{\\partial h_k}$$\n",
    "\n",
    "> The second term of this chain rule is just the derivative of the activation function, which we have chosen to have a conveneint form, while the first term simplifies to:\n",
    "$$\\frac{\\partial E}{\\partial g(h_k)} = \\frac{\\partial}{\\partial g(h_k)}\\left[\\frac{1}{2} \\sum_k (t_k - y_k)^2 \\right] = t_k - y_k$$\n",
    "\n",
    "> Combining these, and assuming (for illustration) a logistic activiation function, we have the gradient:\n",
    "$$\\frac{\\partial E}{\\partial w} = (t_k - y_k) y_k (1-y_k) a_j$$\n",
    "Which ends up getting plugged into the weight update formula that we saw in the single-layer perceptron:\n",
    "$$w_{jk} \\leftarrow w_{jk} - \\eta (t_k - y_k) y_k (1-y_k) a_j$$\n",
    "\n",
    "> Note that here we are *subtracting* the second term, rather than adding, since we are doing gradient descent.\n",
    "\n",
    "> We can now outline the MLP learning algorithm:\n",
    "> 1. Initialize all $w_{jk}$ to small random values\n",
    "> 2. For each input vector, conduct forward propagation:\n",
    "    * compute activation of each neuron $j$ in hidden layer (here, sigmoid):\n",
    "    $$h_j = \\sum_i x_i v_{ij}$$\n",
    "    $$a_j = g(h_j) = \\frac{1}{1 + \\exp(-\\beta h_j)}$$\n",
    "    * when the output layer is reached, calculate outputs similarly:\n",
    "    $$h_k = \\sum_k a_j w_{jk}$$\n",
    "    $$y_k = g(h_k) = \\frac{1}{1 + \\exp(-\\beta h_k)}$$\n",
    "> 3. Calculate loss for resulting predictions:\n",
    "    * compute error at output:\n",
    "    $$\\delta_k = (t_k - y_k) y_k (1-y_k)$$\n",
    "> 4. Conduct backpropagation to get partial derivatives of cost with respect to weights, and use these to update weights:\n",
    "    * compute error of the hidden layers:\n",
    "    $$\\delta_{hj} = \\left[\\sum_k w_{jk} \\delta_k \\right] a_j(1-a_j)$$\n",
    "    * update output layer weights:\n",
    "    $$w_{jk} \\leftarrow w_{jk} - \\eta \\delta_k a_j$$\n",
    "    * update hidden layer weights:\n",
    "    $$v_{ij} \\leftarrow v_{ij} - \\eta \\delta_{hj} x_i$$\n",
    "    \n",
    "> Return to (2) and iterate until learning completes. Best practice is to shuffle input vectors to avoid training in the same order.\n",
    "\n",
    "> Its important to be aware that because gradient descent is a hill-climbing (or descending) algorithm, it is liable to be caught in local minima with respect to starting values. Therefore, it is worthwhile training several networks using a range of starting values for the weights, so that you have a better chance of discovering a globally-competitive solution.\n",
    "\n",
    "> One useful performance enhancement for the MLP learning algorithm is the addition of **momentum** to the weight updates. This is just a coefficient on the previous weight update that increases the correlation between the current weight and the weight after the next update. This is particularly useful for complex models, where falling into local mimima is an issue; adding momentum will give some weight to the previous direction, making the resulting weights essentially a weighted average of the two directions. Adding momentum, along with a smaller learning rate, usually results in a more stable algorithm with quicker convergence. When we use momentum, we lose this guarantee, but this is generally seen as a small price to pay for the improvement momentum usually gives.\n",
    "\n",
    "> A weight update with momentum looks like this:\n",
    "$$w_{jk} \\leftarrow w_{jk} - \\eta \\delta_k a_j + \\alpha \\Delta w_{jk}^{t-1}$$\n",
    "where $\\alpha$ is the momentum (regularization) parameter and $\\Delta w_{jk}^{t-1}$ the update from the previous iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Activation Functions are a kind of function, which helps a Neuron to fire itself in a certain direction, in a  certain bandwidth, or in a certain range. It also try to restrict some sort of dataset.\n",
    "    \n",
    "> Activation functions helps to determine the output of a Neural Network. These type of functions are attached to each Neurons in the Network, and determines whether it should be activated or not, based on whether each neuron’s input is relevant for the model’s prediction. \n",
    "\n",
    "> Activation Function also helps to normalize the output of each neuron to a range between (0 and 1).\n",
    "\n",
    "> Neural Networks use non-linear Activation Functions, which can help the network learn complex data, compute and learn almost any function representing a question, and provide accurate predictions.\n",
    "\n",
    "> The activation function is a mathematical “gate” in between the input feeding the current neuron and its output going to the next layer. It can be as simple as a step function that turns the neuron output on and off, depending on a rule or threshold.\n",
    "\n",
    "<img src='images/1.jpg' width='400px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  There are 3 different types of Activation Functions available to be used in Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Activation functions**\n",
    "\n",
    "> **Linear Activation functions** always defines a Linear relationship  'Y = m.X + c'. Differentials of a Linear function gives some constant, so it can be used in Back-propagation. But a linear activation function is not able to handle the complexity of data like as it will get failed in cases of Image recognitions or more. \n",
    "     \n",
    "<img src='images/linear.png' width='240px'>\n",
    "     \n",
    "**Binary Step Activation functions**\n",
    "\n",
    "> **Binary Step Activation functions** always gives it's output in a range (0 and 1). For Negative X, it always gives 0 and for Positive X, i gives 1. It can handle 2 classes.\n",
    " But it's derivative is always 0, so this function is not proper for using in Back-propagation.\n",
    "     \n",
    "<img src='images/binary-step.png' width='380px'>\n",
    "     \n",
    " **Non-linear Activation functions**\n",
    "     \n",
    ">  Non Linear Activation functions are able to understand the complexity of dataset and also it gives some differentials, which gets used in Back-propagation to update the parameters. So We generally uses Non-linear Activation functions in Deep learning.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####  Some popular Activation functions used in Deep Learning\n",
    "\n",
    "     * Sigmoid Function                        * Tanh Function\n",
    "     * ReLU Function                           * Leaky RelU Function\n",
    "     * PReLU Function                          * ELU Function\n",
    "     * Softmax Function                        * SoftPlus Function\n",
    "     * Swish Function                          * Maxout Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Sigmod  or Logistic Activation Function\n",
    "\n",
    "> The Sigmoid function is the most frequently used Activation function in the beginning of Deep learning.\n",
    "> For every Input value, it gives Output in a range ∈ (0,1). \n",
    "<img src='images/sig.svg' width='200px'>\n",
    "\n",
    "\n",
    "####  Calculation of Derivative of Sigmoid function\n",
    "\n",
    "<img src='images/sigmoid-derivative.jpg' width='480px'>\n",
    "<img src='images/sigmoid-derivative.png' width='320px'>\n",
    "\n",
    "\n",
    "### Graph of Sigmoid Function and it's Derivative\n",
    "\n",
    "<img src='images/sig.jpg' width='480px'>\n",
    "\n",
    "> Advantages of Sigmoid Function : -\n",
    "\n",
    "1. Smooth gradient, preventing “jumps” in output values.\n",
    "2. Output values bound between 0 and 1, normalizing the output of each neuron.\n",
    "3. Clear predictions, i.e very close to 0 or 1.\n",
    "\n",
    "\n",
    "> Disadvantages of Sigmoid Function : -\n",
    "\n",
    "* Leads to Vanishing Gradient problem\n",
    "* Leads to Gradient Saturation or dispersing\n",
    "* Function output is not zero-centered\n",
    "* Time consuming  and Slower for computers due to using Exponential operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Tanh or Hyperbolic Tangent Function\n",
    "\n",
    "> Tanh or Hyperbolic Tangent function is modified version of the Sigmoid Function. \n",
    "The output interval of tanh is [-1 & 1] and the whole function is 0-centric, which is better than Sigmoid.\n",
    "<img src='images/tan.svg' width='200px'>\n",
    "<img src='images/sig-tan.jpg' width='480px'>\n",
    "\n",
    "> Tanh function can be represented in terms of Sigmoid function.\n",
    "<img src='images/tan-sigmoid.jpg' width='420px'>\n",
    "\n",
    "####  Derivative of TanH Function\n",
    "\n",
    "> For calculating the derivative of Tanh Function, we uses u/v rule. \n",
    "<img src='images/tan.jpg' width='240px'>\n",
    "<img src='images/tan-derivative.jpg' width='420px'>\n",
    "\n",
    "> In general Binary classification problems, Tanh function is used for the hidden layer and the Sigmoid function is used for the output layer. However, these are not static, and the specific activation function to be used must be analyzed according to the specific problem, or it depends on debugging.\n",
    "\n",
    "> Comparing to Sigmoid Function, Tanh is a Zero centerd function but still it leads to Vanishing Gradient problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vanishing Gradient Problem\n",
    "\n",
    "> In Back Propagation, Derivatives of each previous layers have some contribution in updating the weights of next layers and We uses Chain rule to update the weights. \n",
    "\n",
    "> The derivatives of Sigmoid Functions ranges between (0 to 0.25)and when Sigmoid functions are getting used as Activation functions in Deep Neural Networks, it's Derivative are getting decreases during updating of weights in each layers. In case of a Deep neural networks, When Back-propagation reaches to the last layer, Derivative of Sigmoid Function gives a very small value (near to 0) and so there will be very minute change or no changes in the weights in the last layer of Network. This problem is known as Vanishing Gradient problem.  \n",
    "\n",
    "> In 1980s or 1990s,, Researchers were not able to create a Deep Neural Network because of this Vanishing Gradient problems.  Sigmoid function was the only Activation function, that people uses and they don't have too many layers in their Neural networks. They just deal with 2 or 3 layered Neural networks to avoiding this Vanishing Gradient problem.\n",
    "\n",
    "> Visit [Blog](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)\n",
    "        [wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
    "        \n",
    "###  How Can Vanishing Gradient Problem be Prevented?\n",
    "\n",
    " > The <b> Rectified Linear Unit (ReLU) </b> activation function can be used as opposed to the <b>Sigmoid</b> or <b>Hyperbolic Tangent function</b> to prevent the Vanishing Gradient problem.\n",
    "\n",
    "> If you choose the <b> ReLU </b> activation function or any of it’s variants as an Activation function, it is recommended to use <b>He initialisation strategy</b> for the network parameters.\n",
    "\n",
    " ### Why Does The Exploding Gradient Problem Occur?\n",
    "\n",
    "> The reason behind Exploding Gradients problem is similar to that of Vanishing gradients problem. The gradients associated with each weight in the network is equal to a product of many numbers. If this is a product of values greater than one it may cause the gradients to become extremely large.\n",
    "\n",
    "> In contrast to the Vanishing gradients problem, exploding gradients occur as a result of the weights in the network but not the activation function.\n",
    "\n",
    "> The weights in the lower layers are more likely to be affected by exploding gradients as their associated gradients are products of more values. This may cause the gradients of the lower layers to become unstable leading to the algorithm diverging.\n",
    "\n",
    "> In case of Exploding Gradients problem, older weights and newer updated ones will completely different and varies a lot and so after each and every epochs <b> Gradient descent</b> will never converge to a point and will just keep jumping here and there. This is where, it is vexry much important to initialize the weights in a proper way. \n",
    "\n",
    "###  How Can The Exploding Gradient Problem Be Prevented?\n",
    "\n",
    "> Gradient clipping may be employed in order to address the exploding gradients problem. This Gradient clipping does not allow gradients to surpass a specified threshold value. This prevents gradients from becoming overly large while still allowing the updates to occur in the correct direction.\n",
    "\n",
    "> [For Gradient clipping, Click Here](https://machinelearningjourney.com/index.php/2020/11/30/gradient-clipping/)\n",
    "\n",
    "###  Non Zero - Centric Problem\n",
    " \n",
    "> Functions having it's centre lying on the origin are termed as Zero - Centric Functions.\n",
    "    Sigmoid function is not a Zero - Centric Function.\n",
    "\n",
    ">  When we uses a Zero - Centric function as Activation Function, it  will try to control the changes or updates in gradients in backward direction. \n",
    "\n",
    ">  In case of a Non Zero-centric Function, Gradient updates will goes in different - different directions and it will make our optimisation much more harder because it is not able to maintain the consistency in Gradient updates and so every time it's value fluctuates and gives a different value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Rectified  Linear  Unit  or  ReLU  Function\n",
    "\n",
    "> ReLU or Rectified Linear Unit function is defined as a function that takes the maximum value and outputs in a range (0, +infinity), and this function is not fully interval-derivable, but we can take sub-gradients.  \n",
    "<img src='images/relu-fn.jpg' width='420px'>\n",
    "<img src='images/relu-der.jpg' width='420px'>\n",
    "<img src='images/relu.png' width='520px'>\n",
    "\n",
    "\n",
    "> Advantages of ReLU Function.\n",
    "*  Overcome with Gradient Saturation or Vanishing Gradient problem. When the input is Positive, it always gives a Gradient but in case of Negative input, it gives Y as 0, so no any Gradient.\n",
    "\n",
    "* ReLU Function not activates all Neurons at a time. Means that, If any Linear combination of inputs are negative, then it will restrict that Neuron to fire itself and gives output as 0. May be in Next iteration, if there will be Positive input, then it will Fire the Neuron. This property is very important in Feed Forward Connection. \n",
    "\n",
    "*  ReLU calculation speed is much faster because it has only a Linear relationship (forward or backward) and so it is much faster than Sigmod and Tanh. (Sigmod and tanh need to calculate the exponent, which results in slower calculation speed.)\n",
    "\n",
    "> Disadvantages of ReLU Function\n",
    " *  When the input is Negative, ReLU is completely inactive, which means that once a negative number is entered, ReLU will die. In this way, in the forward propagation process, it is not a problem. Some areas are sensitive and some are insensitive. But in the back propagation process, if you enter a negative number, the gradient will be completely zero, which has the same problem as the Sigmoid function and Tanh function.\n",
    "\n",
    " *  ReLU Function is not a Zero Centric function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Leaky  ReLU  Function\n",
    "\n",
    "> Leaky ReLU Function is a modified version of ReLU Function. For Negative Inputs, It gives some values of Y.\n",
    "In order to solve the Dead ReLU Problem, people proposed to set the first half of ReLU 0.01x instead of 0.\n",
    "\n",
    ">  Leaky ReLU has all the advantages of ReLU, plus there will be no problems with Dead ReLU, but in actual operation, it has not been fully proved that Leaky ReLU is always better than ReLU. \n",
    "\n",
    "\n",
    "<img src='images/lrelu.svg' width='240px'>\n",
    "<img src='images/leaky-relu.png' width='480px'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PRelu (Parametric ReLU)\n",
    "\n",
    "> Parametric ReLU or PReLU Function is improvised version of ReLU Function. In the negative region, PReLU has a small slope, which can also avoid the problem of ReLU death. Compared to ELU, PReLU is a linear operation in the negative region. Although the slope is small, it does not tend to 0, which is a certain advantage.\n",
    "\n",
    "<img src='images/Capture.PNG' width='480px'>\n",
    "\n",
    "> In PRelu, alpha 'α' is a learnable parameter, which it learns by tuning..\n",
    "\n",
    "* if α = 0, f becomes ReLU\n",
    "* if α > 0, f becomes Leaky ReLU\n",
    "* if α is a learnable parameter, f becomes PReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ELU (Exponential Linear Units) function\n",
    "\n",
    "> ELU is also proposed to solve the problems of ReLU. \n",
    "\n",
    "<img src='images/elu.svg' width='280px'>\n",
    "<img src='images/elu.jpg' width='520px'>\n",
    "\n",
    "\n",
    ">  Advantages of ELU Function\n",
    "\n",
    "* No dead Neurons issues, if input is Negative\n",
    "* ELU is a Zero-centric Function means that Mean of the output is close to 0.\n",
    "\n",
    ">  Disadvantages of ELU Functions\n",
    "\n",
    "*  For negative inputs, ELU computation is expensively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "\n",
    "> Softmax function is used for Multi class classifications in output layers. It is used for getting the probabilities for different multiple classes among all. It always gives an output in a range [0,1].\n",
    "\n",
    "####  Softmax Function is defined as \n",
    "<img src='images/soft.png'>\n",
    "\n",
    "> Softmax is different from the normal max function: the max function only outputs the largest value, and Softmax ensures that smaller values have a smaller probability and will not be discarded directly. It is a \"max\" that is \"soft\".\n",
    "\n",
    "> The denominator of the Softmax function combines all factors of the original output value, which means that the different probabilities obtained by the Softmax function are related to each other.\n",
    "\n",
    "> In the case of binary classification, for Sigmoid, there are:\n",
    "<img src='images/soft1.png'>\n",
    "\n",
    "\n",
    "> For Softmax with K = 2, there are:\n",
    "<img src='images/soft2.png'>\n",
    "\n",
    "> It can be seen that in the case of binary classification, Softmax is degraded to Sigmoid.\n",
    "<img src='images/soft4.png' width='240px'>\n",
    "   \n",
    "###  Calculation of probabilities using Softmax Function  \n",
    "\n",
    "<img src='images/prob-softmax.jpg' width='720px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/soft31.png' width='580px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Swish (A Self-Gated) Function\n",
    "\n",
    ">  Swish Function is derived from the Sigmoid Function itself and it is defined as =>  **y = x * sigmoid (x)**\n",
    "\n",
    "> Swish's design was inspired by the use of sigmoid functions for gating in LSTMs and Highway networks. We use the same value for gating to simplify the gating mechanism, which is called **self-gating**. \n",
    "\n",
    "> The advantage of self-gating is that it only requires a simple scalar input, while normal gating requires multiple scalar inputs. This feature enables self-gated activation functions such as Swish to easily replace activation functions that take a single scalar as input (such as ReLU) without changing the hidden capacity or number of parameters.\n",
    "\n",
    "<img src='images/swish.png' width='420px'\n",
    "     >\n",
    "\n",
    "####  Advantages of Swish Function\n",
    "\n",
    "*  Unboundedness (unboundedness) is helpful to prevent gradient from gradually approaching '0' during slow training, causing Saturation. At the same time, being bounded has advantages, because bounded active functions can have strong regularization, and larger negative inputs will be resolved.\n",
    "\n",
    "* At the same time, smoothness also plays an important role in optimization and generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SoftPlus Activation Function\n",
    "\n",
    "> The softplus function is similar to the ReLU function, but it is relatively smooth.It is unilateral suppression like ReLU. It has a wide acceptance range (0, + inf).\n",
    "\n",
    "<img src='images/softPlus.jpg' width='360px'>\n",
    "<img src='images/softplus.png' width='520px'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Maxout Activation Function\n",
    "\n",
    "> The Maxout activation function don't goes with the linear combination inputs, but it gives the maximumm input among all different inputs coming from various Neurons.\n",
    "<img src='images/max.jpeg' width='240px'>\n",
    "\n",
    "> One relatively popular choice is the Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version. Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have w1,b1 =0).The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks\n",
    "\n",
    "\n",
    "> The Maxout activation is a generalization of the ReLU and the leaky ReLU functions. It is a learnable activation function.\n",
    "\n",
    "> Maxout can be seen as adding a layer of activation function to the deep learning network, which contains a parameter k. Compared with ReLU, sigmoid, etc., this layer is special in that it adds k neurons and then outputs the largest activation value. value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generally speaking, these activation functions have their own advantages and disadvantages. There is no statement that indicates which ones are not working, and which activation functions are good. All the good and bad must be obtained by experiments.\n",
    "\n",
    "> [Blog](https://medium.com/@omkar.nallagoni/activation-functions-with-derivative-and-python-code-sigmoid-vs-tanh-vs-relu-44d23915c1f4#:~:text=Like%20the%20sigmoid%20function%2C%20one%20of%20the%20interesting,of%20tanh%20%28z%29%3A%20a%3D%20%28e%5Ez-e%5E%20%28-z%29%29%2F%20%28e%5Ez%2Be%5E%20%28-z%29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cost Functions used in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  L1 and L2 loss \n",
    "\n",
    "> *L1* and *L2* are two common Loss functions which are mainly used to minimize the error.\n",
    "\n",
    "   *   **L1 loss function** is also known as **Least Absolute Deviations** in short **LAD**.\n",
    "\n",
    "   *   **L2 loss function** is also known as **Least Square Errors** in short **LSE**.\n",
    "\n",
    "### L1 Loss function\n",
    "\n",
    "> It is used to minimize the error which is the sum of all the absolute differences in between the Actual value and the Predicted value.\n",
    "<img src=\"images\\img13.png\" width='480px'>\n",
    "\n",
    "\n",
    "### L2 Loss Function\n",
    "\n",
    "> It is also used to minimize the error which is the sum of all the squared differences in between the Actual value and the Predicted value.\n",
    "<img src=\"images\\img15.png\" width='480px'>\n",
    "\n",
    "> **L1 Loss is more sensitive to outliers and controls them, so it is always advisable to use L1 Loss in case of outliers, otherwise go with L2 Loss.** \n",
    "\n",
    "###  Python Implementation for L1 and L2 losses\n",
    "\n",
    ">  **l1_loss = tf.abs((y_pred - y_actual))**\n",
    "  \n",
    ">  **l2_loss = tf.square((y_pred - y_actual))**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Huber Loss \n",
    "\n",
    "> Huber Loss is a combination of L1 & L2 losses and this uses a threshold value to detect the outliers.\n",
    "  In case of outliers, it uses L1 Loss with some regularizations otherwise it uses the L2 loss. \n",
    "\n",
    "<img src=\"images\\huber-loss.jpg\" width='480px'>  \n",
    "\n",
    "\n",
    "> Huber Loss is often used in Regression problems. Compared with L2 loss, Huber Loss is less sensitive to outliers (because if the residual is too large, it is a Piecewise function, loss is a linear function of the residual).\n",
    "Among them, $\\delta$ is a set parameter, $y$ represents the real value, and $f(x)$ represents the predicted value.\n",
    "\n",
    "> The advantage of this is that when the residual is small, the loss function is L2 norm, and when the residual is large, it is a linear function of L1 norm.\n",
    "\n",
    "> The Huber loss is quadratic when the error is smaller than a thres‐hold δ (typically 1), but linear when the error is larger than δ. Thismakes it less sensitive to outliers than the mean squared error, andit is often more precise and converges faster than the mean abso‐lute error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Huber Loss function \n",
    "\n",
    "> A smooth approximation of Huber loss to ensure that each order is differentiable.\n",
    "\n",
    "<img src=\"images\\img2.png\" width='780px'>\n",
    "\n",
    "> Where $\\delta$ is the set parameter, the larger the value, the steeper the linear part on both sides.\n",
    "\n",
    "<img src=\"images\\img3.png\" width='360px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Hinge Loss\n",
    " \n",
    "> Hinge loss is often used for Binary classification problems, such as ground true: t = 1 or -1, predicted value y = wx + b. \n",
    " \n",
    "> In other words, the closer the y is to t, the smaller the loss will be.\n",
    "\n",
    "<img src='images/hinge-loss.png'>\n",
    "\n",
    "> For getting more about [Hinge Loss ](https://en.wikipedia.org/wiki/Hinge_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cross Entropy Loss\n",
    "\n",
    "> Cross Entropy Loss is mainly applied to Binary classification problems. The predicted value is a probability value and the loss is defined according to the cross entropy. Note the value range of the above value: the predicted value of y should be a probability and the value range is [0,1] .\n",
    "\n",
    "<img src=\"images\\cross-entropy.jpg\" width='420px'>\n",
    "\n",
    "<img src=\"images\\img7.png\" width='420px'>\n",
    "<img src=\"images\\img8.png\" width='420px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sigmoid Cross-entropy Loss\n",
    "\n",
    "> The above cross-entropy loss requires that the predicted value is a probability. Generally, we calculate $scores = x*w + b$. Entering this value into the sigmoid function can compress the value range to (0,1).\n",
    "\n",
    "<img src=\"images\\img9.png\">\n",
    "\n",
    "> It can be seen that the sigmoid function smoothes the predicted value(such as directly inputting 0.1 and 0.01 and inputting 0.1, 0.01 sigmoid and then entering, the latter will obviously have a much smaller change value), which makes the predicted value of sigmoid-ce far from the label loss growth is not so steep.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Softmax Cross-entropy Loss\n",
    "\n",
    "> First, the softmax function can convert a set of fraction vectors into corresponding probability vectors. Here is the definition of softmax function\n",
    "\n",
    "<img src=\"images\\img10.png\">\n",
    "\n",
    "> As above, softmax also implements a vector of 'squashes' k-dimensional real value to the [0,1] range of k-dimensional, while ensuring that the cumulative sum is 1.\n",
    "\n",
    "> According to the definition of cross entropy, probability is required as input.Sigmoid-cross-entropy-loss uses sigmoid to convert the score vector into a probability vector, and softmax-cross-entropy-loss uses a softmax function to convert the score vector into a probability vector.\n",
    "\n",
    "> According to the definition of cross entropy loss.\n",
    "\n",
    "<img src=\"images\\img11.png\">\n",
    "\n",
    "> where $p(x)$ represents the probability that classification $x$ is a correct classification, and the value of $p$ can only be 0 or 1. This is the prior value $q(x)$ is the prediction probability that the $x$ category is a correct classification, and the value range is (0,1)\n",
    "\n",
    "> So specific to a classification problem with a total of C types, then $p(x_j)$, $(0 <_{=} j <_{=} C)$ must be only 1 and C-1 is 0(because there can be only one correct classification, correct the probability of classification as correct classification is 1, and the probability of the remaining classification as correct classification is 0)\n",
    "\n",
    "> Then the definition of softmax-cross-entropy-loss can be derived naturally.\n",
    "\n",
    "<img src=\"images\\img12.png\">\n",
    "\n",
    ">Where $f_j$ is the score of all possible categories, and $f_{y_i}$ is the score of ground true class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Epochs vs Itearations\n",
    "\n",
    "> A complete cycle of sending data to a Neural network and calculating the loss, then updating the weights in backward direction is known as an **epoch**. Or in other words, a Feed forward connection and a Backward Propagation is togetherly known as an **epoch**.\n",
    "\n",
    "> **Epochs** and **Iterations** are similar things, just get used in different context.\n",
    "\n",
    "> **Epochs** is a hyperparameter, getting tuned based on the need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Optimisers in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Optimisers played a vital role in Neural Networks training. When data enters inside the network with Input Nodes, Feed Forward Connection starts and it will end up with getting some losses.\n",
    "\n",
    "> Then Backward Propagation starts and it updates the randomly selected parameters layer-by-layer in the Network, by using some 'optimization' mechanism in backward direction.\n",
    "   \n",
    "> Gradient descent is one of the most popular algorithms to perform optimization and by far the\n",
    "most common way to optimize neural networks. Gradient descent is a way to minimize an objective function J(θ) parameterized by a model’s parameters θ ∈ R<sup>d</sup> by updating the parameters in the opposite direction of the gradient of the  objective function ∇<sub>θ</sub>J(θ) w.r.t. to the parameters. The learning rate η determines the size of the steps we take to reach a (local) minimum.\n",
    "   \n",
    "<img src=\"images\\diag.jpg\" width='720px'>\n",
    "\n",
    ">  There are three variants of gradient descent, which differ in how much data we use to compute the\n",
    "gradient of the objective function. Depending on the amount of data, we make a trade-off between\n",
    "the accuracy of the parameter update and the time it takes to perform an update.\n",
    " \n",
    "     *  Batch Gradient Descent or (BGD)\n",
    "     *  Stochastic Gradient Descent or (SGD)\n",
    "     *  Mini Batch Gradient Descent or (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Batch Gradient Descent\n",
    "\n",
    "> Vanilla Gradient Descent, aka Batch Gradient Descent uses the entire Training set to calculate the **Gradients** of the **Cost function** to the parameters.\n",
    "  \n",
    "> In BGD, We try to send the whole dataset at a time and every records enters to the network one-by-one. Then we calculate a Combined Loss or Cost by taking Summation of all individual losses, and then we send this **Cost function** for optimisation for finding out the Gradients. \n",
    "\n",
    "> It will keep on sending data inside the network and keeps on calculating their **Error values**. Unless and until, it is not going to pass every records & going to calculate the summation of **Error values** for every **'n'** records, it is not going to change the **Weights & Biases**.\n",
    "\n",
    "> It never allows us to update our model with new on-fly dataset or doing **online learning**. **online learning** means I will keep on getting data and will keep on learning those things.\n",
    "\n",
    "###  <center> Equations to update the Parameters using BGD </center>\n",
    "<img src='images/bgd.png' width = '280px'>\n",
    "\n",
    "\n",
    "> **Batch Gradient Descent** can converge to a **Global minimum** for convex functions and to a **Local minimum** for non-convex functions.\n",
    "\n",
    "> In BGD, There will be a single iteration, in which whole dataset is getting passed thru the network and will updates the parameters. We can set the Epochs value from our side.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##  Code implementation for BGD\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    grad = eval_grad(loss_function, data, params)\n",
    "    params = params — learning_rate*grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Disadvantages of BGD\n",
    "* Memory Consumption is too high\n",
    "* Calculation of Gradients will be slow\n",
    "* Computationally very expensive.\n",
    "* Not good to update the model with data on-fly . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stochastic Gradient Descent\n",
    "\n",
    "> **Stochastic Gradient Descent (SGD) in contrast performs a parameter update for each training example\n",
    "x <sup>(i)</sup> and label y <sup>(i)</sup>** .\n",
    "\n",
    "> In **Stochastic Gradient Descent**, a single record is selected and send to the **Neural networks**. We calculates the Loss for this single record and send it for **optimisation** and then update the weights.\n",
    " Then in next Iterations, some other records are sent to the network individually and we calculates the loss and weights are get updated.\n",
    " \n",
    "###  <center> Equations to update the Parameters using SGD </center>\n",
    "<img src='images/Sgd.png' width = '360px'>\n",
    " \n",
    " \n",
    "> In SGD, **Loss function** and **optimisers** are not supposed to wait for the entire dataset to calculate themselves. \n",
    "\n",
    "> Comparing to BGD, SGD works just with a single iteration and so requires less computational resources but takes more time to train the network.\n",
    "\n",
    "> IN BGD, it will get converged to **Local Minima** point, but in SGD, it will always oscillates or vary between one point to other for each and every dataset, so it's very difficult to get **Absolute Minima** point.\n",
    "\n",
    "> IN SGD, gets multiple Minimum values for entire dataset and we get some **zig-zag** type curve and minima's will keep fluctuating.\n",
    "\n",
    "> For SGD, **Learning rate, η** should be lesser comparing to BGD and MBGd.\n",
    "\n",
    "> SGD is faster than BGD and MBGD.\n",
    "\n",
    "\n",
    " \n",
    " **<center>Fluctuations in SGD</center>** \n",
    "<center> <img src='images/sgd.png' width='320px'></center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Code implementation for SGD\n",
    "\n",
    "for i in range ( num_epochs ):\n",
    "    np.random.shuffle (data)\n",
    "    for example in data:\n",
    "        params_grad = evaluate_gradient(loss_function, example, params)\n",
    "        params = params - learning_rate*params_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantages:**\n",
    "\n",
    "> However, because SGD is updated more frequently, the cost function will have severe oscillations.\n",
    "BGD can converge to a local minimum, of course, the oscillation of SGD may jump to a better local minimum.\n",
    "\n",
    "> When we decrease the learning rate slightly, the convergence of SGD and BGD is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Mini-batch Gradient Descent\n",
    "\n",
    "> In MBGD, We make **'n** batches **(n ε (50 ～ 256 advisable)** records and then these mini-batches are selected randomly and enters to the Neural network one-by-one. Then we calculates Costs for each mini-batches and send for **optimisation**.\n",
    "\n",
    "> In a single Iteration, one mini-batch passes thru the Network and we calculate the Cost and send it for **optimisation** and then update the parameters. Then in next Iteration, some other mini-batches will be passes and we calculates the cost and update the weights.\n",
    "\n",
    "###  <center> Equations to update the Parameters using MGD </center>\n",
    "<img src='images/mgd.png' width='420px'>\n",
    " \n",
    "> MBGD uses a small batch of samples, that is, n samples to calculate each time. In this way, it can reduce the variance when the parameters are updated, and the convergence is more stable.\n",
    "\n",
    "> In case of MBGD, iterations will be same as the number of batches, where as number of epochs can be given by our side.\n",
    "\n",
    "> **MBGD is modified version of BGD but with multiple mini-batches with 'n ε (50 ～ 256)' records.\n",
    "\n",
    "> **In MBGD, if batch size is equal to Total no. of records in datset, it becomes BGD.**\n",
    "\n",
    "> **Compare to BGD, MBGD is resource efficient, consumes low memory and calculation is faster. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Code implementation for MGD\n",
    "\n",
    "for i in range (num_epochs):\n",
    "    np.random.shuffle(data)\n",
    "    for batch in get_batches(data, batch_size=50):\n",
    "        params_grad = evaluate_gradient(loss_function, batch, params)\n",
    "        params = params - learning_rate*params_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantages with Mini-Batch Gradient Descent**\n",
    "\n",
    "> **Mini-batch Gradient Descent** does not gives a guarantee that we will be able to do a good convergence of Data or Error in a better way, causes due to randomly selection of batches, because Samples are extracted from dataset don't represents the properties of Entire dataset and so we don't get a good Convergence and so also not a **Absolute Global Minima or Local Minima** points.\n",
    "\n",
    "> If **Learning rate, η** is tto small, my convergence rate falls and Time to take for finding out the Absolute minima will increase and in case of a high **Learning rate, η**, it will not be able to achieve Absolute Mnima and it will keep oscillating between Maximum values and minimum values, causes due to different errors getting for different mini-batches.  \n",
    "\n",
    "> We should control the **Learning rate, η** in case of MBGD.\n",
    "> In addition, this method is to apply the **same learning rate** to all parameter updates. If our data is sparse, we would prefer to update the features with lower frequency.\n",
    "\n",
    "> In addition, for **non-convex functions**, it is also necessary to avoid trapping at the local minimum or saddle point, because the error around the saddle point is the same, the gradients of all dimensions are close to 0, and SGD is easily trapped here.\n",
    "\n",
    "> **Saddle points** are the curves, surfaces, or hypersurfaces of a saddle point neighborhood of a smooth function are located on different sides of a tangent to this point.\n",
    "For example, this two-dimensional figure looks like a saddle: it curves up in the x-axis direction and down in the y-axis direction, and the saddle point is (0,0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Newton's method\n",
    "\n",
    "> It is an iterative way to find out the differentials of any quantity unless and until, it is not going to be a constant or zero.\n",
    "\n",
    "<img src=\"images/newton's-method.jpg\" width=360 px>\n",
    "\n",
    "> Here, calculating **Double derivatives**, We are converging in a more amount. This is what the advantage, that has been used inside newton's method, where we are trying to do smoothning as well as trying to find out the gradients not only just on the baseis of ** 1st order derivatives**, even based on **2nd order derivativs** and findin out the new positions of weights and biases.\n",
    "\n",
    "> [Newton's Method for Back-Propagation paper](https://core.ac.uk/download/pdf/217185101.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Concept of Momentum in Deep Learning </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suppose I am going to kick a ball from a surface on height 'h', that ball will follow some trajectory and falls down to the  earth's surface because of the earth's Gravitational pull with some accelaration.\n",
    " \n",
    "<img src=\"images/trajectory.png\">\n",
    " \n",
    "> When we have kicked up the ball, it starts getting an acceleration and this acceleration costs it to increase the velocity due to the earth's gravity. During the fall, at every point of time, there will be a Tangential force, which will be applied and will try to pull it towards the earth. \n",
    "\n",
    "> During the fall, at every steps, it will keep on changing the velocity (Centripetal velocity as well as Centrifugal velocity) and at the same point of time, it will try to change it's tangents as well. Whenever, it is going to strike on the earth's surface, so at that point of time, it's velocity will be maximum.\n",
    " \n",
    "> Now, During the fall, When we used to calculate the velocities at any of the particular stage, we always try to consider the previous velocities and then calculates the new one. Here in case of optimization, we are supposed to fall in to the minima and then will calculate the parameters.\n",
    "\n",
    "> So far so forth, whenever we have changed the weights, we were considering the older weights and then finding out 1st order differentials and multiplying it with the Learning rate. But the Learning rate was fixed their and was  not an actual learnable parameter. It's a kind of constant parameter, that we were giving from our end in case of all of the vanilla versions of Gradient Deescent algorithms, whether it is BGD, or SGD or it is MGD or in a Newton's method.\n",
    "\n",
    "> In further improved versions of these optimizers, people have introduced a concept of **Momentum**, which simply means that whenever, we are talking about the **Learning rate**, my **Learning rate** will be decided based on the past learnable parameters. \n",
    "\n",
    "> If I am trying to change the weights and maybe if I have changed for the 1st time, then changed for 2nd time and again have changed for 3rd time or 4th time. So from very beginning, I have started with keeping a high **Learning rate**, then decreses in 2nd iteration, again decreses in 3rd iteration and again decreases further based on the learning or based on the parameters that i have tuned in earlier iterations. So here, we are trying to control the **Learning rate** with respect to the **momentum**. \n",
    "\n",
    "> **momentum** means here the new velocity will be decided on the basis of older ones or the changes made into older velocities. If changes are huge, so again our new velocity will get impacted by that. That's a reason whenever a projectile is going to follow a trajectory, everytime, velocity is not going to be same. It will get increasing, because everytime it is feeling 9.8 m/s<sup>2</sup> accelearation and so getting pulled towards earth. It will try to change the previous changes as well, so basically it will always try to consider a momentum of previous stages. \n",
    "\n",
    "> So here also, If we were trying to consider those things, in that case, i can say that my optimiser  will  be a better optimiser and keeping those things in mind, people have developed different-different kind of optimisers based on the concept of momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SGD with Momentum\n",
    "\n",
    "> **SGD** has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one\n",
    "dimension than in another, which are common around local optima. In these scenarios, SGD\n",
    "oscillates across the slopes of the ravine while only making hesitant progress along the bottom\n",
    "towards the local optimum.\n",
    "\n",
    "<img src=\"images/sgd-ravines.png\" width=720px>\n",
    "\n",
    "> **Momentum** is a method that helps to accelerate SGD in the relevant direction and dampens\n",
    "oscillations. It does this by adding a fraction **γ** of the update vector of the\n",
    "past time step to the current update vector. The momentum term **γ** is usually set to **0.9**, which means that i am only considering **90%** from older weights instead of taking **100%**.\n",
    "\n",
    "<img src=\"images/mom-eqn.png\" width=360px>\n",
    "\n",
    "> Using **Momentum-based Optimizers**, We get smooth convergence of data and takes very less time to calculate the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Nesterov Accelerated Gradient aka NAG optimiser\n",
    "\n",
    "> A ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We would\n",
    "like to have a smarter ball, which has a notion of where it is going so that it knows to slow down\n",
    "before the hill slopes up again. **Nesterov Accelerated Gradient (NAG)** is a way to give our momentum term this kind of thing.\n",
    "\n",
    "> We uses our momentum term **γ.v<sub>t−1</sub>** to move the parameters **θ**. Computing <b>θ−γ.v<sub>t−1</sub></b> gives us an approximation of the next position of the parameters a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not with respect to our current parameters **θ** but with respect to the approximate future position of our parameters.\n",
    "\n",
    "<center><b> Equation used by NAG optimiser </b> </center>\n",
    "<img src=\"images/nag.png\" width=480px>\n",
    "</br>\n",
    "\n",
    "> Again, we set the momentum term **γ** to a value of around **0.9**. While Momentum first computes the\n",
    "current gradient (small blue vector line) and then takes a big jump in the direction of the\n",
    "updated accumulated gradient (big blue vector).\n",
    "\n",
    "> NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks.\n",
    "\n",
    "> Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn,\n",
    "we would also like to adapt our updates to each individual parameter to perform larger or smaller\n",
    "updates depending on their importance.\n",
    "\n",
    "<img src=\"images/nag1.png\" width=720px>\n",
    "<center><b>  NAG optimiser Curve </b> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Momentum-based Gradient Descent optimiser or AdaGrad\n",
    "\n",
    "> **Adagrad** is an algorithm for gradient-based optimization, which adapts the **Learning rates** to the parameters, using low **Learning rate** for parameters associated with frequently occurring features, and using high **Learning rates** for parameters associated with infrequent features. So, it is well-suited for dealing with **sparse data**.\n",
    "\n",
    "> **Sparse data** is a kind of data, where we have lot's of mixed data or mixtures of data or in other words, data is not even and there is no any common pattern in b/w the data. So in case of **sparse data**, **AdaGrad** works well.\n",
    "\n",
    "> In very beginning, We starts from a high **Learning rate** and gradually as I will increase my Iterations, I will keep decreasing my **Learning rate** because in very beginning, our **Neural network** will not be aware about dataset and Every time we receives new variances or records of data, but in Later stage, once my **Neural network** is able to pass data for certain epochs (or Iterations) and so my model has alreday adjusted the weights and if their will be some changes in features are not frequent, we will slow down the **Learning rate**.\n",
    "In this case, I am providing my model a better possibility to converge for optimising the Losses.\n",
    "\n",
    "> I will be able to define the **Accuracy** of my model, if it will converge and this way, we uses to keep changing the **Learning rate** and will not use a constant **Learning rate** throughout the entire learning period.\n",
    "\n",
    "> A constant **Learning rate** may not be suitable for all parameters. For example, some parameters may have reached the stage where only fine-tuning is needed, but some parameters need to be adjusted a lot due to the small number of corresponding samples.\n",
    "\n",
    "> Adagrad proposed as an algorithm that adaptively assigns different **Learning rates** to various parameters among them. \n",
    "\n",
    ">**GloVe word embedding uses AdaGrad where infrequent words required a greater update and frequent words require smaller updates.**\n",
    "\n",
    ">**AdaGrad eliminates the need to manually tune the learning rate.**\n",
    "\n",
    "<img src='images/adagrad.jpg' width='480px'>\n",
    "<img src='images/adagrad2.jpg' width='480px'>\n",
    "<img src='images/adagrad1.jpg' width='480px'>\n",
    "\n",
    "\n",
    "\n",
    "##  Disadvantages in Adagrad optimiser\n",
    "\n",
    "* **The learning rate is monotonically decreasing.**\n",
    "* **The learning rate in the late training period is very small.**\n",
    "* **It requires manually setting a global initial learning rate.**\n",
    "* **Computationally very Expensive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta\n",
    "\n",
    "> **Adadelta** is an extension of **Adagrad** and it tries to reduce Adagrad’s property of aggressively and monotonically reducing the **Learning rate**. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size 'w'.\n",
    "\n",
    "> In **AdaGrad optimiser**, we uses summation of squares of gradients from all previous epochs, but In **Adadelta optimiser**, we uses **root-mean-square** of gradient of this current epoch and uses **Learning rate** from previous epoch. \n",
    "\n",
    ">In **Adadelta**, we take the ratio of the **Running average** of the previous epochs to the current gradient and we do not need to set the default **Learning rate**.\n",
    "\n",
    "> So,this way! I am able to eliminate the dependencies of selecting a Hard-coded value as **Learning rate** and this **Learning rate** is dynamic and will not be decreased to a small value because learning rate will be changed in each & every epochs.\n",
    "\n",
    "> In case of **AdaGrad**, if the quantity under square-root will be too high, it will be a big problem for you. This entire factor (considered as **Learning rate**) will be zero and in that case, there will be no any changes, that we can perform in weights.\n",
    "\n",
    "> But in **AdaDelta**, we solves this problem by restricting it with applying a thresh-hold value. We will keep on changing the weights, and always try to find out the error differences, but if it is going beyond the thresh-hold value, we will stop it instantly. So beyond that, we will not let it decrease.\n",
    "\n",
    "<img src='images/adadelta.jpg' width='480px'>\n",
    "\n",
    "<img src='images/ada-delta-1.jpg' width='480px'>\n",
    "<img src='images/ada-delta2.jpg' width='480px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RMSProp Optimiser\n",
    "\n",
    "> The full name of RMSProp algorithm is **Root Mean Square Propagation**, which is an adaptive learning rate optimization algorithm, which is an unpublished, adaptive learning rate method proposed by **Geoffrey Hinton** in his lectures over Coursera.\n",
    "\n",
    "> **Geoffrey Hinton** is known as **Father of Deep learning** and get **Alan Turing award** in 2018.\n",
    "\n",
    "> **RMSprop** in fact is identical to the first update vector of Adadelta that we derived above in **AdaDelta**.\n",
    "\n",
    "<img src='images/rms-prop.png' width='380px'>\n",
    "\n",
    "> **RMSProp** tries to resolve Adagrad’s radically diminishing **Learning rates** by using a moving average of the squared gradient. It utilizes the magnitude of the recent gradient descents to normalize the gradient.\n",
    "\n",
    "<img src='images/rmsprop.jpg' width='480px'>\n",
    "\n",
    "> **RMSprop** as well divides the learning rate by an exponentially decaying average of squared gradients.\n",
    "Hinton suggests **γ** to be set to **0.9**, while a good default value for the learning rate **η** is **0.001**.\n",
    "\n",
    "> **Adagrad** will accumulate all previous gradient squares, but **RMSProp** just calculates the corresponding average value, so it can alleviate the problem that the learning rate of the **Adagrad algorithm** drops quickly.\n",
    "\n",
    "> The difference is that **RMSProp** calculates the **differential squared weighted average of the gradients** and this makes the network functions converge faster. \n",
    "\n",
    "> In **RMSProp**, **Learning rate** gets adjusted automatically and it chooses a different learning rate for each parameter.\n",
    "\n",
    "> **RMSProp** divides the learning rate by the average of the exponential decay of squared gradients.\n",
    "\n",
    "> **RMSprop & Adadelta** both have been developed independently around the same time stemming\n",
    "from the need to resolve **Adagrad**’s radically diminishing learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "**Adaptive Moment Estimation optimiser, Adam** is another method that computes adaptive learning rates for each parameter by storing an exponentially decaying average of past squared gradients like **Adadelta** and **RMSprop**.\n",
    "\n",
    "> **Adam** also keeps an exponentially decaying average of past gradients, similar to momentum.\n",
    "\n",
    "> **Adam** can be viewed as a combination of **Adagrad** and **RMSprop** so that **Adagrad** works well on sparse gradients and **RMSProp** works well in online and non-stationary settings repectively.\n",
    "\n",
    "> **Adam** implements the **Exponential moving average of the gradients** to scale the **Learning rate** instead of a simple average as in **Adagrad**. It keeps an exponentially decaying average of past gradients.\n",
    "\n",
    "> **Adam optimizer** is computationally efficient and has very less memory requirement and it is one of the most popular and famous **Gradient Descent based Optimization** algorithms.\n",
    "\n",
    "> **Adam optimizer** comes with **SGD**, by default but also can be used with other. \n",
    "\n",
    "<img src='images/adam.jpg' width='480 px'>\n",
    "<img src='images/adam1.jpg' width='480 px'>\n",
    "<img src='images/adam2.jpg' width=' 360px'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/adam-1.png\">\n",
    "<img src=\"images/adam-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1<sub>st</sub> momentum or Mean of momentum\n",
    "\n",
    "> We are supposed to take all the average of gradients, so whatever gradients, that we have calculated in past. So we are supposed to take the average of all all the gradients and this is how i will be able to define the value of m. It's a simle momentum algorithm, that we are talking aabout.\n",
    "\n",
    "> Momentum simply means that, whenever we are trying to calculate a new value, so we  are supposed to consider all the previous values as well.\n",
    "\n",
    "> so, here m<sub>t</sub> is nothing but, so 'm' till 't-1', means till 't-1' whatever average gradients, that we have, we hare supposed to consider that one.\n",
    "\n",
    "> Gradients are nothing but de/dw, means whatever changes that u have done, so average of that, u are supposed to consider. Multiplied by some constant (Beta_1 or Beta_2), which are just a constant or a regularization factor, that we are suppose to consider at that point of time. b1 and b2 are generally having values close to '1'. (0.9, 0.8, 0.7 etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2<sub>nd</sub> momentum or Un-centored Variances\n",
    "\n",
    "> 2<sub>nd</sub> momentum is nothing but it is going to be Uncentored variances.\n",
    "\n",
    "> Suppose, we are going to move from one point 'X<sub>1</sub>' to another point 'X<sub>2</sub>'. I will start my car at point 'X<sub>1</sub>' and my speed is 0. Now in b/w this route, my speed is not going to be conastant. It could be possible that my speed can vary. So at some point of time, it could be 10, at some point, it could be 20, some point of time 50, then 60, some point of time 30, then again 10 and may be 80 at sopme point of time. \n",
    "\n",
    " > So u will be able to find that my dataset is varying in b/w 0-80, so whatever speed I am able to achieve in b/w these two points, it's a fluctuation in b/w 0-80, that i am abl;e to find out.. So basically, V<sub>t</sub> is a factor, which will try to consider un-centored variances.\n",
    " \n",
    "> So, here in Adam, People said that - \"Ok so why we are trying to divide all the time with a summation of Gradients. this is what we have done in AdaDelta, AdaGrad, RmsProp as well.\"\n",
    " So people have given a logic that no why we are trying to do that? Let's try to introduce this factor as well, because here, whenever u will trying to train any kind of model or whenever u will update the weights.\n",
    " \n",
    "> so training a model simply means that updating the weights and Biases to find out a pattern b/w data.\n",
    "\n",
    "> So People have given a logic that ok so it coul;d be possible that sometime I am supposed to decrease it but not in a same pattern, or sometime I am supposed to increase something, but not inn a same pattern. so basically, it should depends upon the variancews or the kind of data and ther way, it is going to vary till last step, so it should depends upon that as well.\n",
    "\n",
    "> so whenever variances are going to increase, let's suppose we have decayed something by a huge volume, so we are supposed to consider that factor as well and also maybe sometime, we have decayed something by a small volume, so we are supposed to consider that small  volume at that point of time.\n",
    "\n",
    "> so keeping these things in ther mind, People have given a logic that ok fine, this time in this optimizer, what we will do is? we will try to consider an average factor + this variance factor as well of gradients, that we have calculated in the past.\n",
    "\n",
    "> An this is where, people have introduced these two terms Vt and mt. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In AdaDelta, in RMSProp or in AdaGrad, we have directly used the value of 'G', so whatever, Gradients that i will be able to find out, i was trying to multiply with that one.\n",
    "\n",
    "> But in this one, We are not multiplying with G, we are multiplying with the average, that we will be able to find out till  m<sub>t</sub> at current gradient at time 't'.\n",
    "\n",
    "> m<sub>t</sub> = B_1(m<sub>t-1</sub>) + (1 - B_2)g<sub>t</sub>\n",
    "\n",
    "> and in this case, I will be having a much more control or a better control over the changes, which i am going to in older weights. This is the control, that we are suppose to achieve, this is some thing, that we are suppos e to control at any point of time. Because depends upon this, my entire lewarning depends and happens at any point of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaMax Optimiser\n",
    "\n",
    "<img src='images/Adamax.png'>\n",
    "\n",
    "## Nesterov-accelerated Adaptive Moment Estimation aka Nadam Optimiser\n",
    "\n",
    "<img src='images/nadam.png'>\n",
    "<img src='images/nadam-1.png'>\n",
    "<img src='images/nadam-2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [AMsGrad optimizer](https://paperswithcode.com/method/amsgrad)\n",
    "> [DiffGrad optimizer for CNN](https://paperswithcode.com/paper/diffgrad-an-optimization-method-for)\n",
    "> [For more optimizers, Click here](https://paperswithcode.com/search?q_meta=&q_type=&q=optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisions\n",
    "\n",
    "<img src='images/comp.gif' width='480px'>\n",
    "\n",
    "**<center>Figure :- SGD optimization on loss surface contours</center>**\n",
    "\n",
    "<img src='images/sadd.gif' width='480px'>\n",
    "\n",
    "**<center>Figure :- SGD optimization on saddle point</center>**\n",
    "\n",
    "[Official Optimisers paper](https://arxiv.org/pdf/1609.04747.pdf)\n",
    "\n",
    "# How to choose optimizers?\n",
    "\n",
    "- If the data is sparse, use the self-applicable methods, namely Adagrad, Adadelta, RMSprop, Adam.\n",
    "\n",
    "- RMSprop, Adadelta, Adam have similar effects in many cases.\n",
    "\n",
    "- Adam just added bias-correction and momentum on the basis of RMSprop,\n",
    "\n",
    "- As the gradient becomes sparse, Adam will perform better than RMSprop.\n",
    "\n",
    "**Overall, Adam is the best choice.**\n",
    "\n",
    ">SGD is used in many papers, without momentum, etc. Although SGD can reach a minimum value, it takes longer than other algorithms and may be trapped in the saddle point.\n",
    "\n",
    "- If faster convergence is needed, or deeper and more complex neural networks are trained, an adaptive algorithm is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization\n",
    "\n",
    "> To facilitate learning, we typically normalize the initial values of our parameters by initializing them\n",
    "with zero mean and unit variance. As training progresses and we update parameters to different\n",
    "extents, we lose this normalization, which slows down training and amplifies changes as the network\n",
    "becomes deeper.\n",
    "\n",
    "> **Batch normalization** reestablishes these normalizations for every mini-batch and changes are backpropagated through the operation as well. By making normalization part of the model architecture,\n",
    "we are able to use higher learning rates and pay less attention to the initialization parameters. **Batch\n",
    "normalization** additionally acts as a regularizer, reducing (and sometimes even eliminating) the need\n",
    "for **Dropout ratios**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
